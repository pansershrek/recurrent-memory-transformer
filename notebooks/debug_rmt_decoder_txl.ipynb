{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import PreTrainedModel, AutoModelForSequenceClassification, T5ForConditionalGeneration\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import BertForSequenceClassification\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RMTBaseModel(torch.nn.Module):\n",
    "    def __init__(self, base_model, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.set_params(**rmt_kwargs)\n",
    "\n",
    "    def set_params(self, num_mem_tokens, tokenizer, **rmt_config):\n",
    "        self.rmt_config = rmt_config\n",
    "        self.extract_special_tokens(tokenizer)\n",
    "        self.extend_word_embeddings(num_mem_tokens, tokenizer)\n",
    "\n",
    "        self.segment_size = rmt_config['input_size'] - num_mem_tokens - tokenizer.num_special_tokens_to_add()\n",
    "        if 'sep_token' in tokenizer.special_tokens_map:\n",
    "            self.segment_size -= 1\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        memory = self.model.embeddings(self.mem_token_ids)\n",
    "        memory = memory.repeat(input_shape[0], 1, 1)\n",
    "        return memory\n",
    "\n",
    "    def extract_special_tokens(self, tokenizer):\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.special_token_ids = [tokenizer.pad_token_id]\n",
    "        for token in ['cls_token', 'sep_token', 'eos_token', 'bos_token']:\n",
    "            token_id = getattr(tokenizer, f'{token}_id')\n",
    "            if token_id is not None:\n",
    "                self.register_buffer(token, torch.tensor([token_id]))\n",
    "                self.special_token_ids.append(token_id)\n",
    "            else:\n",
    "                setattr(self, token, None)\n",
    "\n",
    "    def extend_word_embeddings(self, num_mem_tokens, tokenizer):\n",
    "            \n",
    "        vocab_size = self.model.config.vocab_size\n",
    "        extended_vocab_size = vocab_size + num_mem_tokens\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        self.register_buffer('mem_token_ids', torch.arange(vocab_size, vocab_size + num_mem_tokens))\n",
    "        self.model.resize_token_embeddings(extended_vocab_size)\n",
    "\n",
    "        special_tokens = tokenizer.special_tokens_map\n",
    "        mem_start_ind = int('cls_token' in special_tokens or 'bos_token' in special_tokens)\n",
    "        self.memory_position = range(mem_start_ind, mem_start_ind + num_mem_tokens)\n",
    "        self.model.embeddings = self.model.get_input_embeddings()\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "       raise NotImplementedError\n",
    "\n",
    "    def pad_and_segment(self, input_ids):\n",
    "        segmented_batch = []\n",
    "        for seq in input_ids:\n",
    "            drop_mask = torch.any(torch.stack([seq == t for t in self.special_token_ids if t is not None]), dim=0)\n",
    "            seq = seq[~drop_mask]\n",
    "            seq = seq[:self.segment_size * self.rmt_config['max_n_segments']]\n",
    "\n",
    "            align = self.rmt_config.get('segment_alignment')\n",
    "            if align in {'right', None}:\n",
    "                split_inds = (list(range(len(seq), 0, -self.segment_size)) + [0])[::-1]\n",
    "            elif align == 'left':\n",
    "                split_inds = list(range(0, len(seq), self.segment_size)) + [len(seq)]\n",
    "            elif align == 'center':\n",
    "                n_seg = math.ceil(len(seq) / self.segment_size)\n",
    "                split_inds = list(range(0, len(seq), math.ceil(len(seq) / n_seg))) + [len(seq)]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            input_segments = [self.pad_add_special_tokens(t, self.rmt_config['input_size']) for t in input_segments]\n",
    "\n",
    "            # add empty segment markers if needed\n",
    "            n_empty_segments = self.rmt_config['max_n_segments'] - len(input_segments)\n",
    "            input_segments = [None] * n_empty_segments + input_segments\n",
    "\n",
    "            segmented_batch.append(input_segments)\n",
    "\n",
    "        segmented_batch = [[sample[seg_num] for sample in segmented_batch] \\\n",
    "                            for seg_num in range(self.rmt_config['max_n_segments'])]\n",
    "        return segmented_batch\n",
    "\n",
    "    def pad_add_special_tokens(self, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def prepare_kwargs(self, segment_input_ids, kwargs):\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "        if sum(non_empty_mask) == 0:\n",
    "            return None, non_empty_mask\n",
    "            \n",
    "        input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "        inputs_embeds = self.model.embeddings(input_ids)\n",
    "\n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        if seg_kwargs.get('labels') is not None:\n",
    "            seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "        seg_kwargs['attention_mask'] = self.get_attention_mask(input_ids)\n",
    "        if seg_kwargs.get('token_type_ids') is not None:\n",
    "            seg_kwargs['token_type_ids'] = self.get_token_type_ids(input_ids)\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "\n",
    "        return seg_kwargs, non_empty_mask\n",
    "\n",
    "    def process_outputs(self, model_outputs, output_attentions, output_hidden_states):\n",
    "        rmt_out = model_outputs[-1]\n",
    "\n",
    "        segment_keys = ['loss']\n",
    "        if output_attentions:\n",
    "            segment_keys.append('attentions')\n",
    "        if output_hidden_states:\n",
    "            segment_keys.append('hidden_states')\n",
    "\n",
    "        extracted = {}\n",
    "        for seg_num, out in enumerate(model_outputs):\n",
    "            for key, value in out.items():\n",
    "                if any([sk in key for sk in segment_keys]):\n",
    "                    extracted[f'{key}_{seg_num}'] = value\n",
    "\n",
    "        if self.rmt_config['sum_loss']:\n",
    "            losses = [out['loss'] for out in model_outputs]\n",
    "            extracted['loss'] = torch.stack(losses).mean(dim=0)\n",
    "\n",
    "        for key, value in extracted.items():\n",
    "            rmt_out[key] = value\n",
    "        \n",
    "        # drop unnecessary hiddens to save memory\n",
    "        if not output_hidden_states:\n",
    "            for key in rmt_out.keys():\n",
    "                if 'hidden_state' in key:\n",
    "                    rmt_out[key] = None\n",
    "\n",
    "        return rmt_out \n",
    "        \n",
    "    def get_token_type_ids(self, tensor):\n",
    "        return torch.zeros_like(tensor)\n",
    "\n",
    "    def get_attention_mask(self, tensor):\n",
    "        mask = torch.ones_like(tensor)\n",
    "        mask[tensor == self.pad_token_id] = 0\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "\n",
    "class RMTDecoderLMHeadMultiSeg(RMTBaseModel):\n",
    "    def set_params(self, num_mem_tokens, tokenizer, **rmt_config):\n",
    "        self.rmt_config = rmt_config\n",
    "        self.extract_special_tokens(tokenizer)\n",
    "        self.create_memory(num_mem_tokens)\n",
    "\n",
    "        self.segment_size = rmt_config['input_size'] - 2 * num_mem_tokens - tokenizer.num_special_tokens_to_add()\n",
    "        if 'sep_token' in tokenizer.special_tokens_map:\n",
    "            self.segment_size -= 1\n",
    "\n",
    "    def create_memory(self, num_mem_tokens):\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        embeddings = self.model.get_input_embeddings()\n",
    "        memory_weights = torch.randn((num_mem_tokens, self.model.config.n_embd)) * embeddings.weight.data.std()\n",
    "        self.register_parameter('memory', torch.nn.Parameter(memory_weights, requires_grad=True))\n",
    "\n",
    "        self.read_memory_position = range(num_mem_tokens)\n",
    "        self.write_memory_position = range(-num_mem_tokens, 0)\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        create_memory = self.training \\\n",
    "                        or self.rmt_config.get('reinit_mem_each_fwd') \\\n",
    "                        or not hasattr(self, 'memory_state') \\\n",
    "                        or self.rmt_config['max_n_segments'] == 1 \n",
    "        if create_memory:\n",
    "            memory = self.memory.repeat(input_shape[0], 1, 1)\n",
    "        else:\n",
    "            memory = self.memory_state[:input_shape[0]]\n",
    "        return memory\n",
    "    \n",
    "    def detach_memory(self, seg_num):\n",
    "        k2, max_n_segments = self.rmt_config.get('k2'), self.rmt_config.get('max_n_segments')\n",
    "        if seg_num == 0 \\\n",
    "            or k2 in {-1, None} \\\n",
    "            or seg_num + k2 > max_n_segments:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, labels=None, labels_mask=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        kwargs = {'attention_mask': attention_mask, 'token_type_ids': token_type_ids,\n",
    "                  'position_ids': position_ids, 'inputs_embeds': inputs_embeds,\n",
    "                  'labels_mask': labels_mask, #'pos_weight': pos_weight,\n",
    "                  'labels': labels, 'output_attentions': output_attentions,\n",
    "                  'output_hidden_states': output_hidden_states, 'return_dict': return_dict,\n",
    "                  }\n",
    "\n",
    "        memory = self.set_memory(input_ids.shape)\n",
    "        segmented = self.pad_and_segment(input_ids, labels)\n",
    "\n",
    "        base_model_outputs = []\n",
    "        for seg_num, segment in enumerate(zip(*segmented)):\n",
    "\n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment, memory, kwargs)\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            \n",
    "            if self.detach_memory(seg_num):\n",
    "                memory = memory.detach()\n",
    "\n",
    "            seg_kwargs['inputs_embeds'][:, self.read_memory_position] = memory[non_empty_mask]\n",
    "            seg_kwargs['inputs_embeds'][:, self.write_memory_position] = memory[non_empty_mask]\n",
    "            out = self.model(**seg_kwargs)\n",
    "            base_model_outputs.append(out)\n",
    "            \n",
    "            memory[non_empty_mask] = out.hidden_states[-1][:, self.write_memory_position]\n",
    "\n",
    "        self.memory_state = memory\n",
    "        # return (base_model_outputs, kwargs)\n",
    "        out = self.process_outputs(base_model_outputs, kwargs)\n",
    "        return out\n",
    "    \n",
    "    def pad_and_segment(self, input_ids, labels=None):\n",
    "        segmented_batch = []\n",
    "        segmented_batch_labels = []\n",
    "\n",
    "        if labels is None:\n",
    "            labels = [None] * input_ids.shape[0]\n",
    "        batch_labels = labels\n",
    "        for seq, labels in zip(input_ids, batch_labels):\n",
    "\n",
    "            align = self.rmt_config.get('segment_alignment')\n",
    "            if align in {'right', None}:\n",
    "                split_inds = (list(range(len(seq), 0, -self.segment_size)) + [0])[::-1]\n",
    "            elif align == 'left':\n",
    "                split_inds = list(range(0, len(seq), self.segment_size)) + [len(seq)]\n",
    "            elif align == 'center':\n",
    "                n_seg = math.ceil(len(seq) / self.segment_size)\n",
    "                split_inds = list(range(0, len(seq), math.ceil(len(seq) / n_seg))) + [len(seq)]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            # add empty segment markers if needed\n",
    "            n_empty_segments = self.rmt_config['max_n_segments'] - len(input_segments)\n",
    "            input_segments = [None] * n_empty_segments + input_segments\n",
    "            segmented_batch.append(input_segments)\n",
    "\n",
    "            if labels is not None:\n",
    "                labels_segments = [labels[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "                labels_segments = [None] * n_empty_segments + labels_segments\n",
    "                segmented_batch_labels.append(labels_segments)\n",
    "\n",
    "        segmented_batch = [[sample[seg_num] for sample in segmented_batch]\n",
    "                           for seg_num in range(self.rmt_config['max_n_segments'])]\n",
    "        segmented_batch_labels = [[sample[seg_num] for sample in segmented_batch_labels]\n",
    "                                  for seg_num in range(self.rmt_config['max_n_segments'])]\n",
    "\n",
    "        return segmented_batch, segmented_batch_labels\n",
    "\n",
    "    def prepare_kwargs(self, segment, memory, kwargs):\n",
    "        segment_input_ids, segment_labels = segment\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "        if sum(non_empty_mask) == 0:\n",
    "            return None, non_empty_mask\n",
    "\n",
    "        input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "        inputs_embeds = self.model.get_input_embeddings()(input_ids)\n",
    "        inputs_embeds = torch.cat([memory, inputs_embeds, memory], dim=1)\n",
    "\n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        seg_kwargs['attention_mask'] = self.get_attention_mask(inputs_embeds)\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "        if seg_kwargs['labels'] is not None:\n",
    "            labels = torch.stack([el for el, m in zip(segment_labels, non_empty_mask) if m])\n",
    "            memory_labels = torch.ones((labels.shape[0], self.num_mem_tokens), dtype=labels.dtype, device=labels.device) * -100\n",
    "            seg_kwargs['labels'] = torch.cat((memory_labels, labels, memory_labels), dim=1)\n",
    "            seg_kwargs['labels'][:, self.num_mem_tokens] = -100\n",
    "        seg_kwargs.pop('labels_mask')\n",
    "\n",
    "        return seg_kwargs, non_empty_mask\n",
    "    \n",
    "    def get_attention_mask(self, tensor):\n",
    "        mask = torch.ones(*tensor.shape[:2], dtype=torch.int64).to(tensor.device)\n",
    "        mask[tensor == self.pad_token_id] = 0\n",
    "        return mask\n",
    "    \n",
    "    def process_outputs(self, model_outputs, kwargs):\n",
    "        if self.num_mem_tokens in {0, None}:\n",
    "            full_logits = torch.cat([o.logits for o in model_outputs], dim=1)\n",
    "            truncated_hs = [[lh for lh in o.hidden_states] for o in model_outputs]\n",
    "        else:    \n",
    "            full_logits = torch.cat([o.logits[:, self.num_mem_tokens:-self.num_mem_tokens] for o in model_outputs], dim=1)\n",
    "            truncated_hs = [[lh[:, self.num_mem_tokens:-self.num_mem_tokens] for lh in o.hidden_states] for o in model_outputs]\n",
    "        full_hidden_states = tuple([torch.cat(layer_hs, dim=1) for layer_hs in zip(*truncated_hs)])\n",
    "\n",
    "        rmt_out = CausalLMOutputWithCrossAttentions()\n",
    "        full_labels = kwargs.get('labels')\n",
    "        if full_labels is not None:\n",
    "            shift_labels = full_labels[..., 1:].contiguous()\n",
    "            shift_logits = full_logits[..., :-1, :].contiguous()\n",
    "            flat_labels = shift_labels.view(-1)\n",
    "            flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            \n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            labels_mask = kwargs.get('labels_mask')\n",
    "            if labels_mask is not None:\n",
    "                shift_mask = labels_mask[..., :-1].contiguous()\n",
    "                flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "                flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "                \n",
    "            rmt_out['loss'] = loss_fct(flat_logits, flat_labels)\n",
    "\n",
    "        rmt_out['logits'] = full_logits\n",
    "        segment_keys = ['loss', 'logits']\n",
    "        if kwargs.get('output_attentions'):\n",
    "            segment_keys.append('attentions')\n",
    "        if kwargs.get('output_hidden_states'):\n",
    "            segment_keys.append('hidden_states')\n",
    "            rmt_out['hidden_states'] = full_hidden_states\n",
    "\n",
    "        for seg_num, out in enumerate(model_outputs):\n",
    "            for key, value in out.items():\n",
    "                if any([sk in key for sk in segment_keys]):\n",
    "                    rmt_out[f'{key}_{seg_num}'] = value\n",
    "\n",
    "        return rmt_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "\n",
    "def xl_memory_forward(self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        rmt_parent = None\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n",
    "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    )\n",
    "    use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    if input_ids is not None and inputs_embeds is not None:\n",
    "        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "    elif input_ids is not None:\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        batch_size = input_ids.shape[0]\n",
    "    elif inputs_embeds is not None:\n",
    "        input_shape = inputs_embeds.size()[:-1]\n",
    "        batch_size = inputs_embeds.shape[0]\n",
    "    else:\n",
    "        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "    device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "    if token_type_ids is not None:\n",
    "        token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
    "    if position_ids is not None:\n",
    "        position_ids = position_ids.view(-1, input_shape[-1])\n",
    "\n",
    "    if past_key_values is None:\n",
    "        past_length = 0\n",
    "        past_key_values = tuple([None] * len(self.h))\n",
    "    else:\n",
    "        past_length = past_key_values[0][0].size(-2)\n",
    "    if position_ids is None:\n",
    "        position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
    "        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
    "\n",
    "    # GPT2Attention mask.\n",
    "    if attention_mask is not None:\n",
    "        if batch_size <= 0:\n",
    "            raise ValueError(\"batch_size has to be defined and > 0\")\n",
    "        attention_mask = attention_mask.view(batch_size, -1)\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        attention_mask = attention_mask[:, None, None, :]\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "        attention_mask = (1.0 - attention_mask) * -10000.0\n",
    "\n",
    "    # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "    if self.config.add_cross_attention and encoder_hidden_states is not None:\n",
    "        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "        if encoder_attention_mask is None:\n",
    "            encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "        encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "    else:\n",
    "        encoder_attention_mask = None\n",
    "\n",
    "    # Prepare head mask if needed\n",
    "    # 1.0 in head_mask indicate we keep the head\n",
    "    # attention_probs has shape bsz x n_heads x N x N\n",
    "    # head_mask has shape n_layer x batch x n_heads x N x N\n",
    "    head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n",
    "\n",
    "    if inputs_embeds is None:\n",
    "        inputs_embeds = self.wte(input_ids)\n",
    "    position_embeds = self.wpe(position_ids)\n",
    "    hidden_states = inputs_embeds + position_embeds\n",
    "\n",
    "    if token_type_ids is not None:\n",
    "        token_type_embeds = self.wte(token_type_ids)\n",
    "        hidden_states = hidden_states + token_type_embeds\n",
    "\n",
    "    hidden_states = self.drop(hidden_states)\n",
    "\n",
    "    output_shape = input_shape + (hidden_states.size(-1),)\n",
    "\n",
    "    presents = () if use_cache else None\n",
    "    all_self_attentions = () if output_attentions else None\n",
    "    all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "    all_hidden_states = () if output_hidden_states else None\n",
    "    \n",
    "    # init xl cache\n",
    "    xl_cache = rmt_parent.memory_storage['xl_cache']\n",
    "    xl_cache_size = rmt_parent.rmt_config['xl_cache_size']\n",
    "\n",
    "    if 0 in xl_cache and attention_mask is not None:\n",
    "        layer_attention_mask = torch.cat((attention_mask[:, :, :, :xl_cache[0].shape[1]], attention_mask), dim=-1)\n",
    "    else:\n",
    "        layer_attention_mask = attention_mask\n",
    "    for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n",
    "\n",
    "        # Model parallel\n",
    "        if self.model_parallel:\n",
    "            torch.cuda.set_device(hidden_states.device)\n",
    "            # Ensure layer_past is on same device as hidden_states (might not be correct)\n",
    "            if layer_past is not None:\n",
    "                layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)\n",
    "            # Ensure that attention_mask is always on the same device as hidden_states\n",
    "            if attention_mask is not None:\n",
    "                attention_mask = attention_mask.to(hidden_states.device)\n",
    "            if isinstance(head_mask, torch.Tensor):\n",
    "                head_mask = head_mask.to(hidden_states.device)\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "\n",
    "            # if use_cache:\n",
    "            #     logger.warning(\n",
    "            #         \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "            #     )\n",
    "            #     use_cache = False\n",
    "\n",
    "            def create_custom_forward(module):\n",
    "                def custom_forward(*inputs):\n",
    "                    # None for past_key_value\n",
    "                    return module(*inputs, use_cache, output_attentions)\n",
    "\n",
    "                return custom_forward\n",
    "\n",
    "            outputs = torch.utils.checkpoint.checkpoint(\n",
    "                create_custom_forward(block),\n",
    "                hidden_states,\n",
    "                None,\n",
    "                attention_mask,\n",
    "                head_mask[i],\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "            )\n",
    "        else:\n",
    "            if i in xl_cache:\n",
    "                layer_cache = xl_cache[i]\n",
    "                non_empty_mask = rmt_parent.memory_storage['non_empty_mask']\n",
    "                if non_empty_mask is not None:\n",
    "                    layer_cache = layer_cache[non_empty_mask]\n",
    "                hidden_states = torch.cat((layer_cache, hidden_states), dim=1)\n",
    "\n",
    "            outputs = block(\n",
    "                hidden_states,\n",
    "                layer_past=layer_past,\n",
    "                attention_mask=layer_attention_mask,\n",
    "                head_mask=head_mask[i],\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                encoder_attention_mask=encoder_attention_mask,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "\n",
    "        if i in xl_cache:\n",
    "            hidden_states = hidden_states[:, xl_cache[i].shape[1]:]\n",
    "\n",
    "        # update layer cache\n",
    "        xl_cache[i] = hidden_states\n",
    "        if rmt_parent.num_mem_tokens not in {0, None}:\n",
    "            xl_cache[i] = xl_cache[i][:, :-rmt_parent.num_mem_tokens]\n",
    "        xl_cache[i] = xl_cache[i][:, -xl_cache_size:].detach()\n",
    "\n",
    "        if use_cache is True:\n",
    "            presents = presents + (outputs[1],)\n",
    "\n",
    "        if output_attentions:\n",
    "            all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
    "            if self.config.add_cross_attention:\n",
    "                all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n",
    "\n",
    "        # Model Parallel: If it's the last layer for that device, put things on the next device\n",
    "        if self.model_parallel:\n",
    "            for k, v in self.device_map.items():\n",
    "                if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n",
    "                    hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n",
    "\n",
    "    hidden_states = self.ln_f(hidden_states)\n",
    "\n",
    "    hidden_states = hidden_states.view(output_shape)\n",
    "    # Add last hidden state\n",
    "    if output_hidden_states:\n",
    "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "    if not return_dict:\n",
    "        return tuple(\n",
    "            v\n",
    "            for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n",
    "            if v is not None\n",
    "        )\n",
    "\n",
    "    return BaseModelOutputWithPastAndCrossAttentions(\n",
    "        last_hidden_state=hidden_states,\n",
    "        past_key_values=presents,\n",
    "        hidden_states=all_hidden_states,\n",
    "        attentions=all_self_attentions,\n",
    "        cross_attentions=all_cross_attentions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import copy\n",
    "import re\n",
    "class RMTDecoderXLCache(RMTDecoderLMHeadMultiSeg):\n",
    "    def set_params(self, num_mem_tokens, tokenizer, **rmt_config):\n",
    "        super().set_params(num_mem_tokens, tokenizer, **rmt_config)\n",
    "        self.override_encoder_forward(rmt_config.get('xl_forward_func'))\n",
    "        if rmt_config.get('xl_cache_size'):\n",
    "            self.segment_size -= rmt_config['xl_cache_size']\n",
    "\n",
    "    def override_encoder_forward(self, xl_forward_func):\n",
    "        if xl_forward_func is None:\n",
    "            from rmt_utils.decoder.transformer_xl import xl_forward\n",
    "            xl_forward_func = xl_forward\n",
    "        new_forward = lambda *args, **kwargs: xl_forward_func(*args, **kwargs, rmt_parent=self)\n",
    "        self.model.base_model.forward = types.MethodType(new_forward, self.model.base_model)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                inputs_embeds=None, labels=None, labels_mask=None, output_attentions=None, output_hidden_states=None, return_dict=None):\n",
    "        kwargs = {'attention_mask': attention_mask, 'token_type_ids': token_type_ids,\n",
    "                  'position_ids': position_ids, 'inputs_embeds': inputs_embeds,\n",
    "                  'labels_mask': labels_mask, #'pos_weight': pos_weight,\n",
    "                  'labels': labels, 'output_attentions': output_attentions,\n",
    "                  'output_hidden_states': output_hidden_states, 'return_dict': return_dict,\n",
    "                  }\n",
    "\n",
    "        memory = self.set_memory(input_ids.shape)\n",
    "        segmented = self.pad_and_segment(input_ids, labels)\n",
    "\n",
    "        base_model_outputs = []\n",
    "        self.memory_storage = {'xl_cache': dict(), 'non_empty_mask': None}\n",
    "        for seg_num, segment in enumerate(zip(*segmented)):\n",
    "\n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment, memory, kwargs)\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            \n",
    "            if self.detach_memory(seg_num):\n",
    "                memory = memory.detach()\n",
    "\n",
    "            seg_kwargs['inputs_embeds'][:, self.read_memory_position] = memory[non_empty_mask]\n",
    "            seg_kwargs['inputs_embeds'][:, self.write_memory_position] = memory[non_empty_mask]\n",
    "            out = self.model(**seg_kwargs)\n",
    "            base_model_outputs.append(out)\n",
    "            \n",
    "            self.memory_storage['non_empty_mask'] = non_empty_mask\n",
    "            memory[non_empty_mask] = out.hidden_states[-1][:, self.write_memory_position]\n",
    "\n",
    "        self.memory_state = memory\n",
    "\n",
    "        out = self.process_outputs(base_model_outputs, kwargs)\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_segments = 1\n",
    "# num_mem_tokens = 2\n",
    "# device = torch.device(3)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model_name = 'gpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "rmt_config = {'num_mem_tokens': 0, \n",
    "                'max_n_segments': 1,\n",
    "               #  'segment_alignment': 'right',\n",
    "                'tokenizer': tokenizer,\n",
    "               #  'memory_layers': 'all', \n",
    "               #  'share_memory_layers': True,\n",
    "               #  'reconstruction_loss_coef': 0.1,\n",
    "               #  'offset_position_ids': True,\n",
    "                'xl_cache_size': 64,\n",
    "                'xl_forward_func': xl_memory_forward,\n",
    "                'k1': -1, 'k2': 3,\n",
    "                'segment_ordering': 'regular',\n",
    "                'input_size': 128, \n",
    "                'bptt_depth': -1, \n",
    "                'sum_loss': False,\n",
    "             }\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# rmt = RMTDecoderLMHeadMultiSeg(base_model, **rmt_config)\n",
    "rmt = RMTDecoderXLCache(base_model, **rmt_config)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ã‚¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory\n"
     ]
    }
   ],
   "source": [
    "for n, p in rmt.named_parameters():\n",
    "    if 'memory' in n:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt.eval()\n",
    "# rmt.train()\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = batch['input_ids'].clone()#[:, :512]\n",
    "labels = batch['labels'].clone()#[:, :512]\n",
    "\n",
    "labels_mask = batch['labels_mask']\n",
    "# rmt_out_0 = rmt(input_ids, labels=labels, labels_mask=labels_mask, output_hidden_states=True, output_attentions=True)\n",
    "# rmt_out.loss\n",
    "kwargs = dict(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented = rmt.pad_and_segment(input_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_decoder_txl.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu10/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_decoder_txl.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mlen\u001b[39m(segmented[\u001b[39m0\u001b[39m]), segmented[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mshape, segmented[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape, segmented[\u001b[39m0\u001b[39m][\u001b[39m2\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "len(segmented[0]), segmented[0][0][0].shape, segmented[0][1][0].shape, segmented[0][2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt.memory_storage['xl_cache'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt.memory_storage['xl_cache'][11].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt_out = rmt(**kwargs, output_hidden_states=True)\n",
    "base_model_outputs, kw = rmt(**kwargs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([l.logits.shape[1] for l in base_model_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([l.logits.shape[1] for l in base_model_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([l.hidden_states[0].shape[1] for l in base_model_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[56, 64, 64, 64, 64]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l.hidden_states[0].shape[1] for l in base_model_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 56, 768])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_outputs[0].hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([l.logits.shape[1] for l in base_model_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwarg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 28, 50257])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_outputs[0].logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 68, 50257])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_outputs[1].logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 68, 50257])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_outputs[-1].logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'hidden_states', 'loss_0', 'logits_0', 'hidden_states_0', 'loss_1', 'logits_1', 'hidden_states_1', 'loss_2', 'logits_2', 'hidden_states_2', 'loss_3', 'logits_3', 'hidden_states_3', 'loss_4', 'logits_4', 'hidden_states_4'])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 768])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt.memory_storage['xl_cache'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, torch.Size([2, 504, 768]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rmt_out.hidden_states), rmt_out.hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, torch.Size([2, 12, 768]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rmt_out.hidden_states_0), rmt_out.hidden_states_0[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_cache_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loss', tensor(14.0483, grad_fn=<NllLossBackward>)),\n",
       " ('loss_0', tensor(5.7048, grad_fn=<NllLossBackward>)),\n",
       " ('loss_1', tensor(11.0391, grad_fn=<NllLossBackward>)),\n",
       " ('loss_2', tensor(11.2370, grad_fn=<NllLossBackward>)),\n",
       " ('loss_3', tensor(13.3765, grad_fn=<NllLossBackward>)),\n",
       " ('loss_4', tensor(14.0265, grad_fn=<NllLossBackward>))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(key, rmt_out[key]) for key in rmt_out if 'loss' in key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loss', tensor(11.3409, grad_fn=<NllLossBackward>)),\n",
       " ('loss_0', tensor(2.9731, grad_fn=<NllLossBackward>)),\n",
       " ('loss_1', tensor(2.6483, grad_fn=<NllLossBackward>)),\n",
       " ('loss_2', tensor(2.8106, grad_fn=<NllLossBackward>)),\n",
       " ('loss_3', tensor(2.6017, grad_fn=<NllLossBackward>))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "[(key, rmt_out[key]) for key in rmt_out if 'loss' in key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpt_path = \"../../runs/lm_long/wikitext-2-v1/gpt2/lr5e-05_linear_adamw_wd1e-03_630-128-5x128_mem1_bs32_regular_bptt-5_from_cpt_4-5/run_1/model_best.pth\"\n",
    "cpt = torch.load(cpt_path, map_location='cpu')\n",
    "rmt.load_state_dict(cpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 51, 50257]), 13, torch.Size([1, 51, 768]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt_out.logits.shape, len(rmt_out.hidden_states), rmt_out.hidden_states[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "max_n_segments = 4\n",
    "input_size = 128\n",
    "num_mem_tokens = 32\n",
    "\n",
    "input_seq_len = target_seq_len = max_n_segments * (input_size - 2 * num_mem_tokens)\n",
    "batch_size = 2\n",
    "\n",
    "args = Holder\n",
    "args.max_n_segments = max_n_segments\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.input_prefix = ''\n",
    "args.block_size = None\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def get_lm_datasets(raw_datasets, tokenizer, block_size):\n",
    "    # copy-pasted from https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py\n",
    "\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[text_column_name])\n",
    "\n",
    "    # with accelerator.main_process_first():\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        # num_proc=args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        # load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "    # if args.block_size is None:\n",
    "    # block_size = tokenizer.model_max_length\n",
    "    # if block_size > 1024:\n",
    "    #     logger.warning(\n",
    "    #         \"The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value\"\n",
    "    #         \" of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can\"\n",
    "    #         \" override this default with `--block_size xxx`.\"\n",
    "    #     )\n",
    "    # block_size = 1024\n",
    "    # else:\n",
    "    #     if args.block_size > tokenizer.model_max_length:\n",
    "    #         logger.warning(\n",
    "    #             f\"The block_size passed ({args.block_size}) is larger than the maximum length for the model\"\n",
    "    #             f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "    #         )\n",
    "    #     block_size = min(args.block_size, tokenizer.model_max_length)\n",
    "\n",
    "    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "    def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "        if total_length >= block_size:\n",
    "            total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n",
    "    # for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n",
    "    # to preprocess.\n",
    "    #\n",
    "    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "\n",
    "    # with accelerator.main_process_first():\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        # num_proc=args.preprocessing_num_workers,\n",
    "        # load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "    )\n",
    "\n",
    "    train_dataset = lm_datasets[\"train\"]\n",
    "    valid_dataset = lm_datasets[\"validation\"]\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe02d4648c844a9b86d791a82ad4c972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from lm_experiments_tools.lm_datasets import load_dataset\n",
    "raw_datasets = datasets.load_dataset('wikitext', 'wikitext-2-v1')\n",
    "# train_dataset, _ = get_lm_datasets(raw_datasets, tokenizer, block_size=args.input_seq_len)\n",
    "# _, valid_dataset = get_lm_datasets(raw_datasets, tokenizer, block_size=input_size - 2 * num_mem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-389b922bfc5fe729.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-6067a66e735cfbb1.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-941845a5470f2db7.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-867d7b23bfc3f054.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-fdadcdfe8a04e3dc.arrow\n",
      "Loading cached processed dataset at /home/bulatov/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-18cc4303d429ecb0.arrow\n"
     ]
    }
   ],
   "source": [
    "block_size = input_size - 2 * num_mem_tokens\n",
    "history_size = args.input_seq_len - block_size\n",
    "\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "def group_texts(examples, block_size, history_size=None):\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    if history_size is None:\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    else:\n",
    "        result = {\n",
    "            k: [t[max({0, i - history_size}) : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].map(lambda x: group_texts(x, block_size, history_size), \n",
    "                                        batched=True, desc=f\"Grouping train in chunks of {block_size} and history {history_size}\")\n",
    "valid_dataset = tokenized_datasets[\"validation\"].map(lambda x: group_texts(x, block_size), \n",
    "                                        batched=True, desc=f\"Grouping valid in chunks of {block_size}\")\n",
    "test_dataset = tokenized_datasets[\"test\"].map(lambda x: group_texts(x, block_size), \n",
    "                                        batched=True, desc=f\"Grouping test in chunks of {block_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37869"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[0]['input_ids']), len(valid_dataset[0]['input_ids']), len(test_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 64, 64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[1]['input_ids']), len(valid_dataset[1]['input_ids']), len(test_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 64, 64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[2]['input_ids']), len(valid_dataset[2]['input_ids']), len(test_dataset[2]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 51, 58)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[-1]['input_ids']), len(valid_dataset[-1]['input_ids']), len(test_dataset[-1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(b['input_ids'][::-1]) for b in batch]\n",
    "    labels = [torch.tensor(b['labels'][::-1]) for b in batch]\n",
    "    attention_mask = [torch.tensor(b['attention_mask'][::-1]) for b in batch]\n",
    "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value).T.flip(1)\n",
    "    labels = pad_sequence(labels, padding_value=-100).T.flip(1)\n",
    "    attention_mask = pad_sequence(attention_mask, padding_value=0).T.flip(1)\n",
    "\n",
    "    collated = {'input_ids': input_ids,\n",
    "                'labels': labels, \n",
    "                'attention_mask': attention_mask}\n",
    "    \n",
    "    if input_ids.shape[1] != block_size:\n",
    "        labels_mask = torch.ones_like(input_ids, dtype=bool)\n",
    "        labels_mask[:, :-block_size] = False\n",
    "        collated['labels_mask'] = labels_mask\n",
    "    \n",
    "    return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader for RMT\n",
    "# batch sample i is a continuation of sample i of the previous batch\n",
    "class alignedDataLoader(DataLoader):\n",
    "    def __iter__(self):\n",
    "        # all_inds = np.arange(len(self.dataset))\n",
    "        all_inds = np.arange(len(self.dataset) // self.batch_size * batch_size)\n",
    "        all_inds = all_inds.reshape(batch_size, -1)\n",
    "        for batch_ind in range(all_inds.shape[1]):\n",
    "            batch = [self.dataset[int(ind)] for ind in all_inds[:, batch_ind]]\n",
    "            yield self.collate_fn(batch)\n",
    "\n",
    "\n",
    "kwargs = {'pin_memory': True, 'num_workers': 1}\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                # sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "valid_dataloader = alignedDataLoader(valid_dataset, batch_size=batch_size, \n",
    "                            # sampler=valid_sampler,\n",
    "                                collate_fn=collate_fn, drop_last=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = [train_dataset[i] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)\n",
    "batch = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(gen)\n",
    "batch = next(gen)\n",
    "batch = next(gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen = iter(valid_dataloader)\n",
    "valid_batch = next(valid_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  319,   569, 18354,  7496, 17740,  2873,   764,  2893,   340, 17383,\n",
       "           262,  3210,  3033,   286,   262,  2168,   837,   340,   635, 25289,\n",
       "          3294, 16895,   837,   884,   355,  1642,   262,   983,   517,  1279,\n",
       "          2954,    29,   329,  2168, 29661,   764, 15684, 11915,  1279,  2954,\n",
       "            29,  8835,    73,   280,   290, 26777,  7286, 13704, 13231, 43354,\n",
       "          1111,  4504,   422,  2180, 12784,   837,  1863,   351,   569, 18354,\n",
       "          7496, 17740,  2873,  3437, 33687,  5303, 18024,  6909,   764,   317,\n",
       "          1588,  1074,   286,  8786, 12118,   262,  4226,   764,   383,   983,\n",
       "           705,    82,  4756,  7505,   373, 23568,   416,  1737,   705,    77,\n",
       "           764,   220,   198,   632,  1138,   351,  3967,  4200,   287,  2869,\n",
       "           837,   290,   373, 15342,   416,  1111,  4960,   290,  8830,  9188,\n",
       "           764,  2293,  2650,   837,   340,  2722, 41496,  2695,   837,  1863,\n",
       "           351,   281,  9902,  8313,   287,  3389,   286,   326,   614,   764,\n",
       "           632,   373,   635, 16573,   656, 15911,   290,   281,  2656,  2008,\n",
       "         11034,  2168,   764, 14444,   284,  1877,  4200,   286,   569, 18354,\n",
       "          7496, 17740,  2873,   837,   569, 18354,  7496, 17740,  6711,   373,\n",
       "           407, 36618,   837,   475,   257,  4336, 11059, 11670,   351,   262,\n",
       "           983,   705,    82,  9902,  8313,   373,  2716,   287,  1946,   764,\n",
       "          6343,    13, 44206,   561,  1441,   284,   262,  8663,   351,   262,\n",
       "          2478,   286,   569, 18354,  7496,  1058, 22134,  9303,   329,   262,\n",
       "         14047,   604,   764,   220,   198,   796,   796,  3776,  1759,   796,\n",
       "           796,   220,   198,  1081,   351,  2180,  1279,  2954,    29, 17740,\n",
       "          1830,   837,   569, 18354,  7496, 17740,  6711,   318,   257, 16106,\n",
       "          2597,  2488,    12,    31,  2712,   983,   810,  1938,  1011,  1630,\n",
       "           286,   257,  2422,  4326,   290,  1011,   636,   287, 10566,  1028,\n",
       "          4472,  3386,   764, 18152,   389,  1297],\n",
       "        [33687,  5303, 18024,  6909,   764,   317,  1588,  1074,   286,  8786,\n",
       "         12118,   262,  4226,   764,   383,   983,   705,    82,  4756,  7505,\n",
       "           373, 23568,   416,  1737,   705,    77,   764,   220,   198,   632,\n",
       "          1138,   351,  3967,  4200,   287,  2869,   837,   290,   373, 15342,\n",
       "           416,  1111,  4960,   290,  8830,  9188,   764,  2293,  2650,   837,\n",
       "           340,  2722, 41496,  2695,   837,  1863,   351,   281,  9902,  8313,\n",
       "           287,  3389,   286,   326,   614,   764,   632,   373,   635, 16573,\n",
       "           656, 15911,   290,   281,  2656,  2008, 11034,  2168,   764, 14444,\n",
       "           284,  1877,  4200,   286,   569, 18354,  7496, 17740,  2873,   837,\n",
       "           569, 18354,  7496, 17740,  6711,   373,   407, 36618,   837,   475,\n",
       "           257,  4336, 11059, 11670,   351,   262,   983,   705,    82,  9902,\n",
       "          8313,   373,  2716,   287,  1946,   764,  6343,    13, 44206,   561,\n",
       "          1441,   284,   262,  8663,   351,   262,  2478,   286,   569, 18354,\n",
       "          7496,  1058, 22134,  9303,   329,   262, 14047,   604,   764,   220,\n",
       "           198,   796,   796,  3776,  1759,   796,   796,   220,   198,  1081,\n",
       "           351,  2180,  1279,  2954,    29, 17740,  1830,   837,   569, 18354,\n",
       "          7496, 17740,  6711,   318,   257, 16106,  2597,  2488,    12,    31,\n",
       "          2712,   983,   810,  1938,  1011,  1630,   286,   257,  2422,  4326,\n",
       "           290,  1011,   636,   287, 10566,  1028,  4472,  3386,   764, 18152,\n",
       "           389,  1297,   832,  9048,  1492,  2488,    12,    31,   588, 13043,\n",
       "           351, 15108,  2095, 31725,   837,   351,  3435,  5486, 12387,   832,\n",
       "         21346,  4046, 25037,   290, 12387,   832,  1279,  2954,    29,  2420,\n",
       "           764,   383,  2137, 33226,   832,   257,  2168,   286, 14174, 10566,\n",
       "           837, 11835, 14838,   355,  8739,   326,   460,   307, 12748,  1279,\n",
       "          2954,    29,   832,   290,   302, 21542,   355,   484,   389, 14838,\n",
       "           764,   383,  6339,   284,  1123,  1621]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  796,   569, 18354,  ...,   416,  5609,   511],\n",
       "        [ 8686,  4282,   764,  ...,  2478,  2233,   284]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][:, 1004:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels_mask'][:, 1004:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' = Valkyria Chronicles III = \\n SenjÅ no Valkyria 3 : <unk> Chronicles ( Japanese : æˆ¦å ´ã®ãƒ´ã‚¡ãƒ«ã‚­ãƒ¥ãƒªã‚¢3, lit. Valkyria of the Battlefield 3 ), commonly referred to as Valkyria Chronicles III outside Japan, is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable. Released in January 2011 in Japan, it is the third game in the Valkyria series. <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors, the story runs parallel to the first game and follows the \" Nameless \", a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \". \\n The game began development in 2010, carrying over a large portion of the work done on Valkyria Chronicles II. While it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more <unk> for series newcomers. Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries, along with Valkyria Chronicles II director Takeshi Ozawa. A large team of writers handled the script. The game\\'s opening theme was sung by May \\'n. \\n It met with positive sales in Japan, and was praised by both Japanese and western critics. After release, it received downloadable content, along with an expanded edition in November of that year. It was also adapted into manga and an original video animation series. Due to low sales of Valkyria Chronicles II, Valkyria Chronicles III was not localized, but a fan translation compatible with the game\\'s expanded edition was released in 2014. Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4. \\n = = Gameplay = = \\n As with previous <unk> Chronicles games, Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces. Stories are told through comic book @-@ like panels with animated character portraits, with characters speaking partially through voiced speech bubbles and partially through <unk> text. The player progresses through a series of linear missions, gradually unlocked as maps that can be freely <unk> through and replayed as they are unlocked. The route to each story location on the map varies depending on an individual player\\'s approach : when one option is selected, the other is sealed off to the player. Outside missions, the player characters rest in a camp, where units can be customized and character growth occurs. Alongside the main story missions are character @-@ specific sub missions relating to different squad members. After the game\\'s completion, additional episodes are unlocked, some of them having a higher difficulty than those found in the rest of the game. There are also love simulation elements related to the game\\'s two main <unk>, although they take a very minor role. \\n The game\\'s battle system, the <unk> system, is carried over directly from <unk> Chronicles. During missions, players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected, the player moves the character around the battlefield in third @-@ person. A character can only act once per @-@ turn, but characters can be granted multiple turns at the expense of other characters\\'turns. Each character has a field and distance of movement limited by their Action <unk>. Up to nine characters can be assigned to a single mission. During gameplay, characters will call out if something happens to them, such as their health points ( HP ) getting low or being knocked out by enemy attacks. Each character has specific \" Potentials \", skills unique to each character. They are divided into \" Personal Potential \", which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character, and \" Battle Potentials \", which are grown throughout the game and always grant <unk> to a character. To learn Battle Potentials, each character has a unique \" Masters Table \", a grid @-@ based skill table that can be used to acquire and link different skills. Characters also have Special <unk> that grant them temporary <unk> on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without <unk> his Action Point gauge, the character <unk> can shift into her \" Valkyria Form \" and become <unk>, while Imca can target multiple enemy units with her heavy weapon. \\n Troops are divided into five classes : Scouts, <unk>, Engineers, <unk> and Armored Soldier. <unk> can switch classes by changing their'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch['input_ids'][:, 1004:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8686, 4282,  764,  ...,  262, 2656,  837],\n",
       "        [ 262, 3859, 1445,  ..., 3670,  373, 7867]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(valid_dataloader)\n",
    "batch = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-@ eastern Atlantic Ocean from northern Norway to the Azores and Morocco, not including the Baltic Sea. It is also present in most of the Mediterranean Sea, only missing from the section east of Crete, and along only the north @-@'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch['input_ids'][0][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the <unk> and <unk> powers of nearly every one, <unk> their opponents... \\n The side left <unk> and travelled north where they played <unk> in Masterton. The match was won 10 â€“ 8, and'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch['input_ids'][1][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' west coast of the Black Sea. The <unk> populations are found in the Norwegian <unk> <unk> and <unk>, inside the Arctic Circle. \\n The species can be divided into four genetically distinct populations, one widespread population,'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch['input_ids'][0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the next day they faced Wellington, who they also defeated. The fixture against Wellington was nearly abandoned because Scott and the Wellington Rugby Union could not agree on a venue ; the match went ahead only when the Wellington officials agreed to cede the <unk>'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch['input_ids'][1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  796,   569, 18354,  7496, 17740,  6711,   796,   220,   198,  2311])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  796,   569, 18354,  7496, 17740,  6711,   796,   220,   198,  2311])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels'][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[796, 569, 18354, 7496, 17740, 6711, 796, 220, 198, 2311]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['input_ids'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[796, 569, 18354, 7496, 17740, 6711, 796, 220, 198, 2311]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['labels'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2008])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' = Valkyria Chronicles III = \\n SenjÅ no Valkyria 3 : <unk> Chronicles ( Japanese : æˆ¦å ´ã®ãƒ´ã‚¡ãƒ«ã‚­ãƒ¥ãƒªã‚¢3, lit. Valkyria of the Battlefield 3 ), commonly referred to as Valkyria Chronicles III outside Japan, is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable. Released in January 2011 in Japan, it is the third game in the Valkyria series. <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors, the story runs parallel to the first game and follows the \" Nameless \", a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \". \\n The game began development in 2010, carrying over a large portion of the work done on Valkyria Chronicles II. While it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more <unk> for series newcomers. Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries, along with Valkyria Chronicles II director Takeshi Ozawa. A large team of writers handled the script. The game\\'s opening theme was sung by May \\'n. \\n It met with positive sales in Japan, and was praised by both Japanese and western critics. After release, it received downloadable content, along with an expanded edition in November of that year. It was also adapted into manga and an original video animation series. Due to low sales of Valkyria Chronicles II, Valkyria Chronicles III was not localized, but a fan translation compatible with the game\\'s expanded edition was released in 2014. Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4. \\n = = Gameplay = = \\n As with previous <unk> Chronicles games, Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces. Stories are told through comic book @-@ like panels with animated character portraits, with characters speaking partially through voiced speech bubbles and partially through <unk> text. The player progresses through a series of linear missions, gradually unlocked as maps that can be freely <unk> through and replayed as they are unlocked. The route to each story location on the map varies depending on an individual player\\'s approach : when one option is selected, the other is sealed off to the player. Outside missions, the player characters rest in a camp, where units can be customized and character growth occurs. Alongside the main story missions are character @-@ specific sub missions relating to different squad members. After the game\\'s completion, additional episodes are unlocked, some of them having a higher difficulty than those found in the rest of the game. There are also love simulation elements related to the game\\'s two main <unk>, although they take a very minor role. \\n The game\\'s battle system, the <unk> system, is carried over directly from <unk> Chronicles. During missions, players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected, the player moves the character around the battlefield in third @-@ person. A character can only act once per @-@ turn, but characters can be granted multiple turns at the expense of other characters\\'turns. Each character has a field and distance of movement limited by their Action <unk>. Up to nine characters can be assigned to a single mission. During gameplay, characters will call out if something happens to them, such as their health points ( HP ) getting low or being knocked out by enemy attacks. Each character has specific \" Potentials \", skills unique to each character. They are divided into \" Personal Potential \", which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character, and \" Battle Potentials \", which are grown throughout the game and always grant <unk> to a character. To learn Battle Potentials, each character has a unique \" Masters Table \", a grid @-@ based skill table that can be used to acquire and link different skills. Characters also have Special <unk> that grant them temporary <unk> on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without <unk> his Action Point gauge, the character <unk> can shift into her \" Valkyria Form \" and become <unk>, while Imca can target multiple enemy units with her heavy weapon. \\n Troops are divided into five classes : Scouts, <unk>, Engineers, <unk> and Armored Soldier. <unk> can switch classes by changing their assigned weapon. Changing class does not greatly affect the stats gained while in a previous class. With victory in battle, experience points are awarded to the squad, which are distributed into five different attributes shared by the entire squad, a feature differing from early games\\'method of distributing to different unit types. \\n = = Plot = = \\n The game takes place during the Second Europan War. Gallian Army Squad 422, also known as \" The Nameless \", are a penal military unit composed of criminals, foreign <unk>, and military offenders whose real names are erased from the records and <unk> officially referred to by numbers. <unk> by the Gallian military to perform the most dangerous missions that the Regular Army and Militia will not do, they are nevertheless up to the task, exemplified by their motto, <unk> <unk>, meaning \" Always Ready. \" The three main characters are <unk> Kurt Irving, an army officer falsely accused of treason who wishes to redeem himself ; Ace <unk> Imca, a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and <unk> Riela <unk>, a seemingly <unk> young woman who is unknowingly a descendant of the Valkyria. Together with their fellow squad members, these three are tasked to fight against a mysterious Imperial unit known as Calamity Raven, consisting of mostly Darcsen soldiers. \\n As the Nameless officially do not exist, the upper echelons of the Gallian Army exploit the concept of plausible <unk> in order to send them on missions that would otherwise make Gallia lose face in the war. While at times this works to their advantage, such as a successful incursion into Imperial territory, other orders cause certain members of the 422nd great distress. One such member, <unk>, becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven, attached to the ideal of Darcsen independence proposed by their leader, Dahau. At the same time, elements within Gallian Army Command move to erase the Nameless in order to protect their own interests. <unk> by both allies and enemies, and combined with the presence of a traitor within their ranks, the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort. This continues until the Nameless\\'s commanding officer, Ramsey Crowe, who had been kept under house arrest, is escorted to the capital city of <unk> in order to present evidence <unk> the weary soldiers and expose the real traitor, the Gallian General that had accused Kurt of Treason. \\n <unk> due to these events, and partly due to the major losses in manpower Gallia suffers towards the end of the war with the Empire, the Nameless are offered a formal position as a squad in the Gallian Army rather than serve as an anonymous shadow force. This is short @-@ lived, however, as following Maximilian\\'s defeat, Dahau and Calamity Raven move to activate an ancient <unk> super weapon within the Empire, kept secret by their benefactor. Without the support of Maximilian or the chance to prove themselves in the war with Gallia, it is Dahau\\'s last <unk> card in creating a new Darcsen nation. As an armed Gallian force invading the Empire just following the two nations\\'cease @-@ fire would certainly wreck their newfound peace, Kurt decides to once again make his squad the Nameless, asking Crowe to list himself and all under his command as killed @-@ in @-@ action. Now owing allegiance to none other than themselves, the 422nd confronts Dahau and destroys the <unk> weapon. Each member then goes their separate ways in order to begin their lives <unk>. \\n = = Development = = \\n Concept work for Valkyria Chronicles III began after development finished on Valkyria Chronicles II in early 2010, with full development beginning shortly after this. The director of Valkyria Chronicles II, Takeshi Ozawa, returned to that role for Valkyria Chronicles III. Development work took approximately one year. After the release of Valkyria Chronicles II, the staff took a look at both the popular response for the game and what they wanted to do next for the series. Like its predecessor, Valkyria Chronicles III was developed for PlayStation Portable : this was due to the team wanting to refine the mechanics created for Valkyria Chronicles II, and they had not come up with the \" revolutionary \" idea that would warrant a new entry for the PlayStation 3. Speaking in an interview, it was stated that the development team considered Valkyria Chronicles III to be the series\\'first true sequel : while Valkyria Chronicles II had required a large amount of trial and error during development due to'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in valid_dataloader:\n",
    "    if batch['input_ids'].shape[0] != 2:\n",
    "        print(batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'attention_mask', 'labels']), torch.Size([2, 1014]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys(), batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_decoder.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu11/home/bulatov/bulatov/RMT_light/framework/notebooks/debug_rmt_decoder.ipynb#Y206sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m base_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1047\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1047\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1048\u001b[0m     input_ids,\n\u001b[1;32m   1049\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1050\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1051\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1052\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1053\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1054\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1055\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1056\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1057\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1058\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1059\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1060\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1061\u001b[0m )\n\u001b[1;32m   1062\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1064\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:890\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    880\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    881\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    882\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    888\u001b[0m     )\n\u001b[1;32m    889\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 890\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    891\u001b[0m         hidden_states,\n\u001b[1;32m    892\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    893\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    894\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    895\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    896\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    897\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    898\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    899\u001b[0m     )\n\u001b[1;32m    901\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    902\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:432\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    430\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    431\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 432\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    433\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n\u001b[1;32m    434\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:359\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 359\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_fc(hidden_states)\n\u001b[1;32m    360\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(hidden_states)\n\u001b[1;32m    361\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(hidden_states)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/transformers/modeling_utils.py:1871\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1869\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m   1870\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n\u001b[0;32m-> 1871\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[1;32m   1872\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(size_out)\n\u001b[1;32m   1873\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = base_model(**batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
