{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "# from megatron.data.dataset_utils import get_indexed_dataset_\n",
    "\n",
    "# import horovod.torch as hvd\n",
    "# from dotenv import load_dotenv\n",
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "sys.path.append('../')\n",
    "# from lm_experiments_tools import TrainerArgs\n",
    "from lm_experiments_tools.trainer import Trainer\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# from lm_experiments_tools.lm_datasets import get_lm_datasets\n",
    "from task_utils.contract_nli import process_file\n",
    "import transformers  # noqa: E402\n",
    "from transformers import AutoConfig, AutoTokenizer, HfArgumentParser  # noqa: E402\n",
    "\n",
    "from lm_experiments_tools.utils import collect_run_configuration, get_cls_by_name, get_optimizer  # noqa: E402\n",
    "import lm_experiments_tools.optimizers as optimizers  # noqa: E402\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "#                     level=logging.INFO)\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# # if CUDA_VISIBLE_DEVICES is not set make all gpus visible\n",
    "# if os.environ.get('CUDA_VISIBLE_DEVICES', None) is None:\n",
    "#     os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(i) for i in range(torch.cuda.device_count())])\n",
    "\n",
    "# logger.info(f\"CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}\")\n",
    "# # first call to torch.cuda.device_count() sets visible gpus, following calls will not change the result\n",
    "# logger.info(f\"CUDA DEVICE COUNT: {torch.cuda.device_count()}\")\n",
    "\n",
    "# hvd.init()\n",
    "\n",
    "\n",
    "# # limit # of CPU threads to be used per pytorch worker, otherwise it might use all cpus and throttle gpus\n",
    "# # > 2 fails cause of https://github.com/pytorch/pytorch/issues/56615\n",
    "# # need to upgrade to torch>1.8.1\n",
    "# torch.set_num_threads(4)\n",
    "# # all gpus set with CUDA_VISIBLE_DEVICES are visible to process, indexing from 0 to ...\n",
    "# torch.cuda.set_device(hvd.local_rank())\n",
    "\n",
    "# parser = HfArgumentParser(TrainerArgs)\n",
    "# parser.add_argument('--task_name', type=str, help=\"Task name, wikitext, ...\")\n",
    "# parser.add_argument('--validate_only', action='store_true', default=False,\n",
    "#                     help='Skip training and run only validation. (default: False)')\n",
    "# parser.add_argument('--working_dir', type=str, default='.',\n",
    "#                     help='working dir, should be a dir with t5-experiments repo (default: .)')\n",
    "# parser.add_argument('--seed', type=int, default=42, help='random seed')\n",
    "# parser.add_argument('--show_valid_examples', type=int, default=0,\n",
    "#                     help='how many valid examples to show during training (default: 0)')\n",
    "# parser.add_argument('--input_seq_len', type=int, default=128, help='input sequnce length (default: 128).')\n",
    "# parser.add_argument('--target_seq_len', type=int, default=16, help='target sequnce length, should be set to '\n",
    "#                                                                    'max(len(target))+1 for EOS (default: 16).')\n",
    "# parser.add_argument('--data_n_workers', type=int, default=2, help='number of dataloader workers (default: 2)')\n",
    "\n",
    "# parser.add_argument('--input_prefix', type=str, default='', help='add task prefix to an input string (default: \"\")')\n",
    "# parser.add_argument('--sliding_window', action='store_true', help='use slinding window attentinon mask, '\n",
    "#                     'eval on last segment only', default=False)\n",
    "\n",
    "# # model args\n",
    "# parser.add_argument('--from_pretrained', type=str, help='model name in HF Model Hub (default: \"\")')\n",
    "# parser.add_argument('--model_cfg', type=str, help='path to model configuration file (default: \"\")')\n",
    "# parser.add_argument('--model_cls', type=str, default='transformers:BertForPreTraining',\n",
    "#                     help='model class name to use (default: transformers:BertForPreTraining)')\n",
    "# parser.add_argument('--memory_cell_cls', type=str, default=None, help='cell class for RMT')\n",
    "# parser.add_argument('--recurrent_wrapper_cls', type=str, default=None, help='recurrent wrapper class for RMT')\n",
    "# parser.add_argument('--model_cpt', type=str, default=None, help='pretrained model checkpoint path')\n",
    "# parser.add_argument('--backbone_cls', type=str, default=None,\n",
    "#                     help='backbone class name to use for RMT')\n",
    "# parser.add_argument('--model_type', type=str, default='encoder-decoder',\n",
    "#                     help='model type, encoder, encoder-decoder, decoder, affects preprocessing '\n",
    "#                          '(default: encoder-decoder)')\n",
    "\n",
    "\n",
    "# # Aydar # RMT args \n",
    "# parser.add_argument('--input_size', type=int, default=None, help='maximal input size of the backbone model')\n",
    "# parser.add_argument('--num_mem_tokens', type=int, default=None, help='number of memory tokens.')\n",
    "# parser.add_argument('--max_n_segments', type=int, default=1, help='maximal segment number')\n",
    "# # parser.add_argument('--sum_loss', action='store_true', default=False,\n",
    "# #                     help='with this flag task loss from all segments is summed')\n",
    "# # parser.add_argument('--bptt_depth', type=int, default=-1, help='max number of previous segments in gradient computation.')\n",
    "# # parser.add_argument('--segment_ordering', type=str, help='segment order', default='regular',\n",
    "# #                     choices=['regular', 'reversed', 'bidirectional', 'repeat_first', 'last_memory_only'])\n",
    "# # parser.add_argument('--memory_forward_func', type=str, help='path to memory forward funÑtion script', default=None)\n",
    "# # parser.add_argument('--memory_layers', type=str, help='memory-augmented layer inds or \"all\" for all layers', default=None)\n",
    "# # parser.add_argument('--share_memory_layers', action='store_true', help='share weights of memory layers', default=False)\n",
    "# # parser.add_argument('--reconstruction_loss_coef', type=float, default=None,\n",
    "# #                     help='reconstuction loss ratio in total loss')\n",
    "# # # parser.add_argument('--segment_ordering', type=str,help='????', default='regular',\n",
    "# # #                     choices=['regular', 'reversed', 'bidirectional', 'repeat_first', 'last_memory_only'])\n",
    "# # parser.add_argument('--retain_graph', action='store_true', help='Retain computation graph during backward pass', default=False)\n",
    "# # parser.add_argument('--use_truncated_backward', action='store_true', default=False,\n",
    "# #                     help='whether to use RMT truncated bptt method in backward')\n",
    "# # parser.add_argument('--k1', type=int, default=-1, help='(not implemented) If not -1, gradient update is done each k1 segments')\n",
    "# parser.add_argument('--k2', type=int, default=-1, help='number of last segments used by backward')\n",
    "# parser.add_argument('--freeze_model_weights', action='store_true', default=False,\n",
    "#                     help='Stop training all model weights except memory layers')\n",
    "# parser.add_argument('--backbone_cpt', type=str, default=None, help='backbone model checkpoint path')\n",
    "\n",
    "\n",
    "# # tokenizer\n",
    "# # todo: add wordpiece tokenizers support?\n",
    "# parser.add_argument('--tokenizer', type=str, default=None, help='path or name of pre-trained HF Tokenizer')\n",
    "\n",
    "# # optimizer args\n",
    "# parser.add_argument('--optimizer', type=str, default='AdamW', help='optimizer name: AdamW, Adafactor. (default: AdamW)')\n",
    "# parser.add_argument('--weight_decay', type=float, default=0.0, help='optimizer weight decay (default: 0.0)')\n",
    "# parser.add_argument('--scale_parameter', action='store_true', default=False,\n",
    "#                     help='Adafactor scale_parameter (default: False)')\n",
    "# parser.add_argument('--relative_step', action='store_true', default=False,\n",
    "#                     help='Adafactor relative_step (default: False)')\n",
    "# parser.add_argument('--warmup_init', action='store_true', default=False,\n",
    "#                     help='Adafactor warmup_init (default: False)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_metric():\n",
    "    scrolls_metric_path = hf_hub_download(repo_id=\"tau/scrolls\", filename=\"metrics/scrolls.py\", repo_type=\"dataset\")\n",
    "    updated_scrolls_metric_path = (\n",
    "        os.path.dirname(scrolls_metric_path) + os.path.basename(scrolls_metric_path).replace(\".\", \"_\") + \".py\"\n",
    "    )\n",
    "    shutil.copy(scrolls_metric_path, updated_scrolls_metric_path)\n",
    "    return updated_scrolls_metric_path\n",
    "\n",
    "\n",
    "scrolls_metric_path = download_metric()\n",
    "\n",
    "task_to_metric = {\n",
    "    'gov_report': ['rouge/rouge1', 'rouge/rouge2', 'rouge/rougeL', 'rouge/rougeLsum', 'rouge/geometric_mean'],\n",
    "    'summ_screen_fd': ['rouge/rouge1', 'rouge/rouge2', 'rouge/rougeL', 'rouge/rougeLsum', 'rouge/geometric_mean'],\n",
    "    'qmsum': ['rouge/rouge1', 'rouge/rouge2', 'rouge/rougeL', 'rouge/rougeLsum', 'rouge/geometric_mean'],\n",
    "    'narrative_qa': ['f1'],\n",
    "    'qasper': ['f1'],\n",
    "    'quality': ['exact_match'],\n",
    "    'contract_nli': ['exact_match']\n",
    "}\n",
    "\n",
    "tasks_with_duplicates = {'narrative_qa', 'qasper'}\n",
    "\n",
    "\n",
    "# https://github.com/tau-nlp/scrolls/blob/5bfb8dbaf3a0128ac8c65922096fd95a645f6ba2/baselines/src/utils/duplicates.py#L1\n",
    "# some tasks have multiple possible labels for single input, drop_duplicates_in_input will collect such labels\n",
    "def drop_duplicates_in_input(untokenized_dataset):\n",
    "    indices_to_keep = []\n",
    "    id_to_idx = {}\n",
    "    outputs = []\n",
    "    for i, (id_, output) in enumerate(zip(untokenized_dataset[\"id\"], untokenized_dataset[\"output\"])):\n",
    "        if id_ in id_to_idx:\n",
    "            outputs[id_to_idx[id_]].append(output)\n",
    "            continue\n",
    "        indices_to_keep.append(i)\n",
    "        id_to_idx[id_] = len(outputs)\n",
    "        outputs.append([output])\n",
    "    untokenized_dataset = untokenized_dataset.select(indices_to_keep).flatten_indices()\n",
    "    untokenized_dataset = untokenized_dataset.remove_columns(\"output\")\n",
    "    untokenized_dataset = untokenized_dataset.add_column(\"outputs\", outputs)\n",
    "    return untokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return None\n",
    "\n",
    "args = Holder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cpt = '/cephfs/home/bulatov/bulatov/RMT_light/runs/contract_nli/gpt2/linear_adamw_wd1e-03_124-128-1x128_mem2_bs128_regular_bptt-1_from_cpt_0-1/run_1/'\n",
    "# model_cpt = \"/cephfs/home/bulatov/bulatov/RMT_light/runs/contract_nli/gpt2/linear_adamw_wd1e-03_118-128-1x128_mem5_bs128_regular_bptt-1_from_cpt_0-1/run_1\"\n",
    "# model_cpt = '/cephfs/home/bulatov/bulatov/RMT_light/runs/contract_nli/gpt2/lr5e-05_linear_adamw_wd1e-03_236-128-2x128_mem5_bs128_iters1500_regular_bptt-2/run_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(model_cpt + '/config.json', 'r') as f:\n",
    "#     config = json.load(f)\n",
    "\n",
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = (\"--model_path\", \"/home/jovyan/rmt/runs/test/scrolls\",\n",
    "\"--from_pretrained\", \"gpt2\",\n",
    "\"--task_name\", \"qasper\",\n",
    "\"--model_type\", \"decoder\",\n",
    "\"--memory_cell_cls\", \"modeling_rmt.language_modeling:MemoryCell\",\n",
    "\"--recurrent_wrapper_cls\", \"modeling_rmt.language_modeling:RecurrentWrapper\",\n",
    "\"--model_cls\",  \"transformers:AutoModelForCausalLM\",\n",
    "\"--segment_alignment\", \"right\",\n",
    "# \"--model_cpt\", model_cpt,\n",
    "\"--optimizer\", \"AdamW\",\n",
    "\"--weight_decay\", \"0.001\",\n",
    "\"--lr\", \"1e-03\", \n",
    "\"--lr_scheduler\", \"constant_with_warmup\",\n",
    "\"--clip_grad_value\", \"5.0\")\n",
    "\n",
    "int_attrs = (\n",
    "    \n",
    "# \"--input_seq_len\", \"118\",\n",
    "\"--input_seq_len\", \"128\",\n",
    "\"--input_size\", \"128\",\n",
    "\"--target_seq_len\", \"128\",\n",
    "\"--num_mem_tokens\", \"0\",\n",
    "\"--max_n_segments\" ,\"1\", \n",
    "\"--batch_size\", \"2\", \n",
    "\"--gradient_accumulation_steps\", \"1\",\n",
    "\"--iters\", \"100\",\n",
    "\"--num_warmup_steps\", \"100\",\n",
    "\"--data_n_workers\", \"2\",\n",
    "\"--log_interval\", \"10\",\n",
    "\"--show_valid_examples\", \"5\",\n",
    "\"--early_stopping_patience\", \"15\",\n",
    "\"--seed\", \"42\",\n",
    "\"--k2\", \"-1\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, v in zip(int_attrs[::2], int_attrs[1::2]):\n",
    "    setattr(args, a.split('--')[1], int(v))\n",
    "\n",
    "for a, v in zip(attrs[::2], attrs[1::2]):\n",
    "    try:\n",
    "        setattr(args, a.split('--')[1], float(v))\n",
    "    except ValueError:\n",
    "        setattr(args, a.split('--')[1], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# args = parser.parse_args()\n",
    "# set current working dir\n",
    "# args.working_dir = str(Path(args.working_dir).expanduser().absolute())\n",
    "# os.chdir(args.working_dir)\n",
    "# if hvd.rank() == 0:\n",
    "#     logger.info(f'hvd size: {hvd.size()}')\n",
    "#     logger.info(f'FP16: {args.fp16}')\n",
    "\n",
    "# if hvd.rank() == 0 and args.model_path is None:\n",
    "#     logger.warning('model_path is not set: config, logs and checkpoints will not be saved.')\n",
    "\n",
    "# # create model path and save configuration\n",
    "# if hvd.rank() == 0 and args.model_path is not None:\n",
    "#     model_path = Path(args.model_path)\n",
    "#     if not model_path.exists():\n",
    "#         Path(model_path).mkdir(parents=True)\n",
    "#     args_dict = collect_run_configuration(args)\n",
    "#     # todo: if model path exists and there is config file, write new config file aside\n",
    "#     json.dump(args_dict, open(model_path/'config.json', 'w'), indent=4)\n",
    "\n",
    "if not args.from_pretrained:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.from_pretrained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "if args.model_type == 'decoder':\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "    # tokenizer.pad_token_id = tokenizer.eos_token\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': ['[GEN]', '[PAD]']})\n",
    "    gen_token = tokenizer.encode('[GEN]')[0]\n",
    "    tokenizer.pad_token_id = tokenizer.encode('[PAD]')[0]\n",
    "    id_pad_value = tokenizer.pad_token_id\n",
    "\n",
    "    block_size = args.input_size\n",
    "    if args.num_mem_tokens not in {0, None}:\n",
    "        block_size -= 2 * args.num_mem_tokens\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "        if 'outputs' in batch[0]:\n",
    "            # if we have more than 1 label per example (only in valid) take only one of them\n",
    "            # to compute loss on valid\n",
    "            target_text = [b['outputs'][0][:args.input_seq_len * 10] for b in batch]\n",
    "        else:\n",
    "            target_text = [b['output'][:args.input_seq_len * 10] for b in batch]\n",
    "\n",
    "        collated = {}\n",
    "        inputs = tokenizer.batch_encode_plus(list(inputs), max_length=args.input_seq_len, truncation=True, padding=False)\n",
    "        labels = tokenizer.batch_encode_plus(list(target_text), max_length=args.target_seq_len, truncation=True, padding=False)\n",
    "\n",
    "        full_inputs = [torch.tensor(i[:args.input_seq_len - len(l) - 1] + [gen_token] + l) for i, l in zip(inputs['input_ids'], labels['input_ids'])]\n",
    "        full_inputs = pad_sequence(full_inputs, padding_value=tokenizer.pad_token_id).T\n",
    "\n",
    "        gen_inputs = [torch.tensor(i[:args.input_seq_len - len(l) - 1] + [gen_token]) for i, l in zip(inputs['input_ids'], labels['input_ids'])]\n",
    "        gen_inputs = pad_sequence(gen_inputs, padding_value=tokenizer.pad_token_id).T\n",
    "        \n",
    "        labels_mask = torch.zeros_like(full_inputs).bool()\n",
    "        for i, l in enumerate(labels['input_ids']):\n",
    "            labels_mask[i, -len(l) -1:] = True\n",
    "\n",
    "        collated['input_ids'] = collated['labels'] = full_inputs\n",
    "        collated['input_ids_generate'] = gen_inputs\n",
    "        collated['labels_mask'] = labels_mask\n",
    "        collated['attention_mask'] = (full_inputs != id_pad_value).bool()\n",
    "        collated['attention_mask_generate'] = (gen_inputs != id_pad_value).bool()\n",
    "\n",
    "        collated['id'] = [b['id'] for b in batch]\n",
    "        collated['target_text'] = target_text\n",
    "        # for k, v in collated.items():\n",
    "        #     if hasattr(v, 'shape'):\n",
    "        #             print(k, v.shape)\n",
    "        return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get train dataset\n",
    "# if hvd.rank() == 0:\n",
    "#     logger.info(f'preparing dataset for: {args.task_name}')\n",
    "dataset = datasets.load_dataset('tau/scrolls', args.task_name)\n",
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['validation']\n",
    "# shuffle train data each epoch (one loop over train_dataset)\n",
    "# train_sampler = DistributedSampler(train_dataset, rank=hvd.rank(), num_replicas=hvd.size(), shuffle=True,\n",
    "#                                     drop_last=False, seed=args.seed)\n",
    "per_worker_batch_size = int(args.batch_size * args.gradient_accumulation_steps)\n",
    "global_batch_size = int(args.batch_size)\n",
    "kwargs = {'pin_memory': True, 'num_workers': args.data_n_workers}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=per_worker_batch_size, \n",
    "# sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "                                # collate_fn=collate_train, **kwargs)\n",
    "# get validation dataset\n",
    "valid_dataloader = None\n",
    "# if hvd.rank() == 0:\n",
    "#     logger.info(f'preparing validation data from: {args.task_name}')\n",
    "# valid_sampler = DistributedSampler(valid_dataset, rank=hvd.rank(), num_replicas=hvd.size(), shuffle=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=per_worker_batch_size, \n",
    "# sampler=valid_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "                                # collate_fn=collate_valid, **kwargs)\n",
    "\n",
    "# test_sampler = DistributedSampler(test_dataset, rank=hvd.rank(), num_replicas=hvd.size(), shuffle=False)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=per_worker_batch_size, \n",
    "# sampler=test_sampler,\n",
    "                                # collate_fn=collate_fn, **kwargs)\n",
    "# if args.valid_interval is None:\n",
    "#     args.valid_interval = args.log_interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "\n",
    "class MemoryCell(torch.nn.Module):\n",
    "    def __init__(self, base_model, num_mem_tokens):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "        self.create_memory(num_mem_tokens)\n",
    "\n",
    "    def create_memory(self, num_mem_tokens):\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        embeddings = self.model.get_input_embeddings()\n",
    "        memory_dim =  getattr(self.model.config, 'n_embd', self.model.config.hidden_size)\n",
    "        memory_weights = torch.randn((num_mem_tokens, memory_dim)) * embeddings.weight.data.std()\n",
    "        self.register_parameter('memory', torch.nn.Parameter(memory_weights, requires_grad=True))\n",
    "\n",
    "        self.read_memory_position = range(num_mem_tokens)\n",
    "        self.write_memory_position = range(-num_mem_tokens, 0)\n",
    "\n",
    "    def set_memory(self, input_shape):\n",
    "        memory = self.memory.repeat(input_shape[0], 1, 1)\n",
    "        return memory\n",
    "\n",
    "    def forward(self, input_ids, memory_state=None, **kwargs):\n",
    "        if memory_state is None:\n",
    "            memory_state = self.set_memory(input_ids.shape)\n",
    "\n",
    "        seg_kwargs = self.process_input(input_ids, memory_state, **kwargs)\n",
    "        out = self.model(**seg_kwargs)\n",
    "        out, new_memory_state = self.process_output(out, **kwargs)\n",
    "\n",
    "        return out, new_memory_state\n",
    "    \n",
    "    def generate(self, input_ids, memory_state, attention_mask, **generate_kwargs):\n",
    "        if memory_state is None:\n",
    "            memory_state = self.set_memory(input_ids.shape)\n",
    "\n",
    "        seg_kwargs = self.process_input(input_ids, memory_state, attention_mask=attention_mask)\n",
    "        out = self.model.generate(inputs_embeds=seg_kwargs['inputs_embeds'], attention_mask=seg_kwargs['attention_mask'], **generate_kwargs)\n",
    "        return out\n",
    "\n",
    "    def process_input(self, input_ids, memory_state, **kwargs):\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "\n",
    "        inputs_embeds = kwargs.get('inputs_embeds')\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.model.get_input_embeddings()(input_ids)\n",
    "        if self.num_mem_tokens > 0:\n",
    "            inputs_embeds = torch.cat([memory_state, inputs_embeds, memory_state], dim=1)\n",
    "\n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        if kwargs.get('attention_mask') is not None:\n",
    "            seg_kwargs['attention_mask'] = self.pad_attention_mask(kwargs['attention_mask'], inputs_embeds.shape)\n",
    "        seg_kwargs['output_hidden_states'] = True\n",
    "        return seg_kwargs\n",
    "    \n",
    "    def pad_attention_mask(self, attention_mask, shape):\n",
    "        if self.num_mem_tokens in {0, None}:\n",
    "            return attention_mask\n",
    "        else:\n",
    "            mask = torch.ones(*shape[:2], dtype=torch.int64).to(attention_mask.device)\n",
    "            mask[:, self.num_mem_tokens:-self.num_mem_tokens] = attention_mask\n",
    "            return mask\n",
    "    \n",
    "    def process_output(self, model_outputs, **kwargs):\n",
    "        if self.num_mem_tokens not in {0, None}:\n",
    "            out = CausalLMOutputWithCrossAttentions()\n",
    "            memory_state = model_outputs.hidden_states[-1][:, -self.num_mem_tokens:]\n",
    "            out['logits'] = model_outputs.logits[:, self.num_mem_tokens:-self.num_mem_tokens]\n",
    "            \n",
    "            if kwargs.get('output_hidden_states'):\n",
    "                out['hidden_states'] = [lh[:, self.num_mem_tokens:-self.num_mem_tokens] for lh in model_outputs.hidden_states]\n",
    "            if kwargs.get('output_attentions'):\n",
    "                out['attentions'] = model_outputs['attentions']\n",
    "        else:\n",
    "            memory_state = None\n",
    "            out = model_outputs\n",
    "            \n",
    "        return out, memory_state \n",
    "\n",
    "\n",
    "import random\n",
    "class RecurrentWrapper(torch.nn.Module):\n",
    "    def __init__(self, memory_cell, **rmt_kwargs):\n",
    "        super().__init__()\n",
    "        self.memory_cell = memory_cell\n",
    "        self.rmt_config = rmt_kwargs\n",
    "\n",
    "    def forward(self, input_ids, labels=None, labels_mask=None, inputs_embeds=None, attention_mask=None, output_attentions=None, output_hidden_states=None):\n",
    "        memory_state = None\n",
    "        segmented = self.segment(input_ids=input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        # print('\\n\\n\\n\\n\\n\\nsegmented', len(segmented))\n",
    "        # print('input_ids', input_ids.shape)\n",
    "        # print('segmented', segmented[0].shape)\n",
    "\n",
    "        cell_outputs = []\n",
    "        for seg_num, segment in enumerate(segmented):\n",
    "            cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "            cell_outputs.append(cell_out)\n",
    "            self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "        out = self.process_outputs(cell_outputs, labels=labels, \n",
    "                                   labels_mask=labels_mask,\n",
    "                                   output_attentions=output_attentions, \n",
    "                                   output_hidden_states=output_hidden_states)\n",
    "        return out\n",
    "    \n",
    "    def generate(self, input_ids, attention_mask=None, **generate_kwargs):\n",
    "        memory_state = None\n",
    "        segmented = self.segment(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        for seg_num, segment in enumerate(segmented[:-1]):\n",
    "            cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "\n",
    "        final_segment = segmented[-1]\n",
    "        out = self.memory_cell.generate(**final_segment, memory_state=memory_state, **generate_kwargs)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def segment(self, **kwargs):\n",
    "        segments = []\n",
    "        for k, tensor in kwargs.items():\n",
    "            if tensor is not None:\n",
    "                k_segments = self.split_tensor(tensor)\n",
    "                for s, k_seg in enumerate(k_segments):\n",
    "                    if s < len(segments):\n",
    "                        segments[s][k] = k_seg\n",
    "                    else:\n",
    "                        segments.append({k: k_seg})\n",
    "\n",
    "        return segments\n",
    "    \n",
    "    def split_tensor(self, tensor):\n",
    "        align = self.rmt_config.get('segment_alignment')\n",
    "        segment_size = self.rmt_config.get('segment_size')\n",
    "        if align in {'left', None}:\n",
    "            split_inds = list(range(0, tensor.shape[1], segment_size)) + [tensor.shape[1]]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align in {'right', None}:\n",
    "            split_inds = (list(range(tensor.shape[1], 0, -segment_size)) + [0])[::-1]\n",
    "            segments = [tensor[:, start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        elif align == 'center':\n",
    "            n_seg = math.ceil(tensor.shape[1] / segment_size)\n",
    "            segments = torch.chunk(tensor, n_seg, dim=1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return segments\n",
    "\n",
    "    def process_outputs(self, cell_outputs, **kwargs):\n",
    "        out = CausalLMOutputWithCrossAttentions()\n",
    "        full_logits = torch.cat([o.logits for o in cell_outputs], dim=1)\n",
    "        full_hidden_states = tuple([torch.cat(layer_hs, dim=1) for layer_hs in zip(*[o.hidden_states for o in cell_outputs])])\n",
    "\n",
    "        labels = kwargs.get('labels')\n",
    "        if labels is not None:\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            shift_logits = full_logits[..., :-1, :].contiguous()\n",
    "            flat_labels = shift_labels.view(-1)\n",
    "            flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            \n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            labels_mask = kwargs.get('labels_mask')\n",
    "            if labels_mask is not None:\n",
    "                shift_mask = labels_mask[..., :-1].contiguous()\n",
    "\n",
    "                flat_labels = flat_labels[shift_mask.view(-1)]\n",
    "                flat_logits = flat_logits[shift_mask.view(-1)]\n",
    "                \n",
    "            # print('\\n\\n\\n\\nshift_labels: ', shift_labels)\n",
    "            # print('shift_logits: ', shift_logits)\n",
    "            # print('shift_mask: ', shift_mask)\n",
    "            # print([l[m].shape for l, m in zip(shift_labels, shift_mask)])\n",
    "            # if any([sum(m) == 0 for m in shift_mask]):\n",
    "                # print('\\n\\n\\n\\nshift_labels: ', shift_labels)\n",
    "                # print('shift_logits: ', shift_logits)\n",
    "                # print('shift_mask: ', shift_mask)\n",
    "                # raise ValueError\n",
    "            # # 1/0\n",
    "            out['loss'] = loss_fct(flat_logits, flat_labels)\n",
    "            if out['loss'] is None:\n",
    "                raise ValueError\n",
    "        else:\n",
    "            out['loss'] = 0\n",
    "\n",
    "        out['logits'] = full_logits\n",
    "        segment_keys = ['loss', 'logits']\n",
    "        if kwargs.get('output_attentions'):\n",
    "            segment_keys.append('attentions')\n",
    "        if kwargs.get('output_hidden_states'):\n",
    "            segment_keys.append('hidden_states')\n",
    "            out['hidden_states'] = full_hidden_states\n",
    "\n",
    "        for seg_num, o in enumerate(cell_outputs):\n",
    "            for key, value in o.items():\n",
    "                if any([sk in key for sk in segment_keys]):\n",
    "                    out[f'{key}_{seg_num}'] = value\n",
    "\n",
    "        return out \n",
    "        \n",
    "    def manage_gradients(self, memory_state, seg_num):\n",
    "        k2, max_n_segments = self.rmt_config.get('k2'), self.rmt_config.get('max_n_segments')\n",
    "        if seg_num == 0 \\\n",
    "            or k2 in {-1, None} \\\n",
    "            or seg_num + k2 > max_n_segments:\n",
    "                return True\n",
    "        \n",
    "        memory_state = memory_state.detach()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentWrapperCustomForward(RecurrentWrapper):\n",
    "    def __init__(self, *args, **rmt_kwargs):\n",
    "        super().__init__(*args, **rmt_kwargs)\n",
    "        \n",
    "        base_model_forward = rmt_kwargs.get('base_model_forward_func', False)\n",
    "        if not base_model_forward:\n",
    "            raise ValueError(\"'base_model_forward_func' is undefined\")\n",
    "        self.override_base_model_forward(base_model_forward)\n",
    "        self.memory_storage = {}\n",
    "\n",
    "    def forward(self, input_ids, labels=None, labels_mask=None, inputs_embeds=None, attention_mask=None, output_attentions=None, output_hidden_states=None):\n",
    "        memory_state = None\n",
    "        self.memory_storage = {}\n",
    "        segmented = self.segment(input_ids=input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "\n",
    "        cell_outputs = []\n",
    "        for seg_num, segment in enumerate(segmented):\n",
    "            cell_out, memory_state = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "            cell_outputs.append(cell_out)\n",
    "            self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "        out = self.process_outputs(cell_outputs, labels=labels, \n",
    "                                   labels_mask=labels_mask,\n",
    "                                   output_attentions=output_attentions, \n",
    "                                   output_hidden_states=output_hidden_states)\n",
    "        return out\n",
    "\n",
    "    def override_base_model_forward(self, custom_forward):\n",
    "        new_forward = lambda *args, **kwargs: custom_forward(*args, **kwargs, rmt_parent=self)\n",
    "        self.memory_cell.model.gpt_neox.forward = types.MethodType(new_forward, self.memory_cell.model.gpt_neox)\n",
    "\n",
    "    def manage_gradients(self, memory_state, seg_num):\n",
    "        k2, max_n_segments = self.rmt_config.get('k2'), self.rmt_config.get('max_n_segments')\n",
    "        if seg_num == 0 \\\n",
    "            or k2 in {-1, None} \\\n",
    "            or seg_num + k2 > max_n_segments:\n",
    "                return True\n",
    "        \n",
    "        memory_state = memory_state.detach()\n",
    "        for k, t in self.memory_storage.items():\n",
    "            self.memory_storage[k] = t.detach\n",
    "        return False\n",
    "\n",
    "\n",
    "class RecurrentWrapperCustomForwardNoMemPass(RecurrentWrapperCustomForward):\n",
    "    def forward(self, input_ids, labels=None, labels_mask=None, inputs_embeds=None, attention_mask=None, output_attentions=None, output_hidden_states=None):\n",
    "        memory_state = None\n",
    "        self.memory_storage = {}\n",
    "        segmented = self.segment(input_ids=input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "\n",
    "        cell_outputs = []\n",
    "        for seg_num, segment in enumerate(segmented):\n",
    "            cell_out, _ = self.memory_cell(**segment, memory_state=memory_state, output_hidden_states=True)\n",
    "            cell_outputs.append(cell_out)\n",
    "            self.manage_gradients(memory_state, seg_num)\n",
    "\n",
    "        out = self.process_outputs(cell_outputs, labels=labels, \n",
    "                                   labels_mask=labels_mask,\n",
    "                                   output_attentions=output_attentions, \n",
    "                                   output_hidden_states=output_hidden_states)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-11 19:26:19,019] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple\n",
    "from base_models.modeling_gpt_neox import *\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
    "    gather_indices = position_ids[:, None, :, None]  # [bs, 1, seq_len, 1]\n",
    "    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])\n",
    "    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n",
    "    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "def memory_augmented_forward(\n",
    "    self,\n",
    "    hidden_states: torch.FloatTensor,\n",
    "    attention_mask: torch.FloatTensor,\n",
    "    position_ids: torch.LongTensor,\n",
    "    head_mask: Optional[torch.FloatTensor] = None,\n",
    "    layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "    use_cache: Optional[bool] = False,\n",
    "    output_attentions: Optional[bool] = False,\n",
    "    layer_memory_storage=None,\n",
    "    memory_layer=None    \n",
    "):\n",
    "    has_layer_past = layer_past is not None\n",
    "\n",
    "    # Compute QKV\n",
    "    # Attention heads [batch, seq_len, hidden_size]\n",
    "    #   --> [batch, seq_len, (np * 3 * head_size)]\n",
    "    qkv = self.query_key_value(hidden_states)\n",
    "\n",
    "    # [batch, seq_len, (num_heads * 3 * head_size)]\n",
    "    #   --> [batch, seq_len, num_heads, 3 * head_size]\n",
    "    new_qkv_shape = qkv.size()[:-1] + (self.num_attention_heads, 3 * self.head_size)\n",
    "    qkv = qkv.view(*new_qkv_shape)\n",
    "\n",
    "    # [batch, seq_len, num_attention_heads, 3 * head_size] --> 3 [batch, num_attention_heads, seq_len, head_size]\n",
    "    query = qkv[..., : self.head_size].permute(0, 2, 1, 3)\n",
    "    key = qkv[..., self.head_size : 2 * self.head_size].permute(0, 2, 1, 3)\n",
    "    value = qkv[..., 2 * self.head_size :].permute(0, 2, 1, 3)\n",
    "\n",
    "    # Compute rotary embeddings on rotary_ndims\n",
    "    query_rot = query[..., : self.rotary_ndims]\n",
    "    query_pass = query[..., self.rotary_ndims :]\n",
    "    key_rot = key[..., : self.rotary_ndims]\n",
    "    key_pass = key[..., self.rotary_ndims :]\n",
    "\n",
    "    # Compute token offset for rotary embeddings (when decoding)\n",
    "    seq_len = key.shape[-2]\n",
    "    if has_layer_past:\n",
    "        seq_len += layer_past[0].shape[-2]\n",
    "    cos, sin = self.rotary_emb(value, seq_len=seq_len)\n",
    "    query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin, position_ids)\n",
    "    query = torch.cat((query, query_pass), dim=-1)\n",
    "    key = torch.cat((key, key_pass), dim=-1)\n",
    "\n",
    "    # Cache QKV values\n",
    "    if has_layer_past:\n",
    "        past_key = layer_past[0]\n",
    "        past_value = layer_past[1]\n",
    "        key = torch.cat((past_key, key), dim=-2)\n",
    "        value = torch.cat((past_value, value), dim=-2)\n",
    "    present = (key, value) if use_cache else None\n",
    "\n",
    "    # Compute attention\n",
    "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
    "\n",
    "    # Reshape outputs\n",
    "    attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_size)\n",
    "    attn_output = self.dense(attn_output)\n",
    "\n",
    "    outputs = (attn_output, present)\n",
    "    if output_attentions:\n",
    "        outputs += (attn_weights,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-09 12:53:32,191] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50259. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "source": [
    "block_size=args.input_size-2*args.num_mem_tokens\n",
    "# define model\n",
    "model_cls = get_cls_by_name(args.model_cls)\n",
    "# if hvd.rank() == 0:\n",
    "#     logger.info(f'Using model class: {model_cls}')\n",
    "if not args.from_pretrained:\n",
    "    model_cfg = AutoConfig.from_pretrained(args.model_cfg)\n",
    "    model = model_cls(config=model_cfg)\n",
    "else:\n",
    "    # if hvd.rank() == 0:\n",
    "    #     logger.info(f'Loading pretrained model: {args.from_pretrained}')\n",
    "    model = model_cls.from_pretrained(args.from_pretrained)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "## load cpt of backbone model\n",
    "if args.backbone_cpt:\n",
    "    backbone_cpt = os.path.join(args.backbone_cpt, \"model_best.pth\")\n",
    "    cpt = torch.load(backbone_cpt, map_location='cpu')\n",
    "    model.load_state_dict(cpt['model_state_dict'], strict=False)\n",
    "    # if hvd.rank() == 0:\n",
    "    #     logger.info(f'Loaded baseline state dict from: {args.backbone_cpt}')\n",
    "\n",
    "# Pass memory settings to pretrained model\n",
    "if args.num_mem_tokens is not None:\n",
    "    memory_cell_cls = get_cls_by_name(args.memory_cell_cls)\n",
    "    recurrent_wrapper_cls = get_cls_by_name(args.recurrent_wrapper_cls)\n",
    "    # memory_cell_cls = MemoryCell\n",
    "    # recurrent_wrapper_cls = RecurrentWrapper\n",
    "    # if hvd.rank() == 0:\n",
    "    #     logger.info(f'Wrapping in: {memory_cell_cls} and {recurrent_wrapper_cls}')\n",
    "    \n",
    "    \n",
    "    cell = memory_cell_cls(model, args.num_mem_tokens)\n",
    "    model = recurrent_wrapper_cls(cell, \n",
    "                                    segment_size=block_size,\n",
    "                                    max_n_segments=args.max_n_segments, \n",
    "                                    k2=args.k2,\n",
    "                                    segment_alignment=args.segment_alignment\n",
    "    )\n",
    "                                \n",
    "\n",
    "    ## load cpt of rmt\n",
    "    if args.model_cpt:\n",
    "        model_cpt = os.path.join(args.model_cpt, \"model_best.pth\")\n",
    "        cpt = torch.load(model_cpt, map_location='cpu')\n",
    "        model.load_state_dict(cpt['model_state_dict'], strict=False)\n",
    "        # if hvd.rank() == 0:\n",
    "            # logger.info(f'Loaded RMT state dict from: {args.model_cpt}')\n",
    "        print(f'Loaded RMT state dict from: {args.model_cpt}')\n",
    "\n",
    "if args.freeze_model_weights:\n",
    "    for n, p in model.named_parameters():\n",
    "        # if 'memory' not in n and 'wte' not in n:\n",
    "        if 'memory' not in n and 'lora' not in n:\n",
    "            p.requires_grad = False\n",
    "    # if hvd.rank() == 0:\n",
    "    #     logger.info(f'Frozen moodel weights')\n",
    "    #     logger.info(f'Remaining parameters: {[n for n, p in model.named_parameters() if p.requires_grad]}')\n",
    "\n",
    "# # fix the not-contiguous error with loralib and horovod\n",
    "# def make_contiguous(module):\n",
    "#     with torch.no_grad():\n",
    "#         for param in module.parameters():\n",
    "#             param.set_(param.contiguous())\n",
    "# make_contiguous(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define optimizer\n",
    "optimizer_cls = get_optimizer(args.optimizer)\n",
    "if optimizer_cls is None:\n",
    "    raise RuntimeError(f'{args.optimizer} was not found in optimizers, torch.optim, transformers.optimization')\n",
    "\n",
    "# if hvd.rank() == 0:\n",
    "#     logger.info(f'Using optimizer class: {optimizer_cls}')\n",
    "\n",
    "# todo: group optimizer params\n",
    "if optimizer_cls in [transformers.optimization.Adafactor, optimizers.Adafactor]:\n",
    "    # https://github.com/huggingface/transformers/pull/9751/files -> transformers 4.3.0\n",
    "    optimizer = optimizer_cls(model.parameters(), lr=args.lr,\n",
    "                                scale_parameter=args.scale_parameter,\n",
    "                                relative_step=args.relative_step,\n",
    "                                warmup_init=args.warmup_init,\n",
    "                                weight_decay=args.weight_decay)\n",
    "else:\n",
    "    optimizer = optimizer_cls(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "def keep_for_metrics_fn(batch, output):\n",
    "    # select data from batch and model output that would be used to compute metrics\n",
    "    data = {}\n",
    "    if 'generation_outputs' in output:\n",
    "        data['labels'] = batch['target_text']\n",
    "\n",
    "        data['generation_outputs'] = output['generation_outputs']\n",
    "        # if 'labels_mask' in batch:\n",
    "        #     data['generation_outputs'] = [data['generation_outputs'][i, mask] for i, mask in enumerate(batch['labels_mask'])]\n",
    "\n",
    "    for key in batch.keys():\n",
    "        if 'loss' in key: \n",
    "            data[key] = batch[key]\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1161727/1406884141.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  scrolls_metric = datasets.load_metric(scrolls_metric_path, args.task_name, keep_in_memory=True)\n"
     ]
    }
   ],
   "source": [
    "scrolls_metric = datasets.load_metric(scrolls_metric_path, args.task_name, keep_in_memory=True)\n",
    "\n",
    "def metrics_fn(data):\n",
    "    # compute metrics based on stored labels, predictions, ...\n",
    "    metrics = {}\n",
    "    y, p = None, None\n",
    "    if 'generation_outputs' in data:\n",
    "        # replace -100 with pad token in labels\n",
    "        y = data['labels']\n",
    "        p = tokenizer.batch_decode(data['generation_outputs'], skip_special_tokens=True)\n",
    "\n",
    "        metrics['exact_match'] = np.mean([y_ == p_[:len(y_)] for p_, y_ in zip (p, y)])\n",
    "        # if args.show_valid_examples > 0:\n",
    "        #     for i in range(min(args.show_valid_examples, len(y))):\n",
    "        #         logger.info(f'y: {y[i]}')\n",
    "        #         logger.info(f'p: {p[i]}')\n",
    "        #         logger.info(f'p ids: {data[\"generation_outputs\"][i]}')\n",
    "\n",
    "        #         logger.info('-' * 50)\n",
    "        \n",
    "        if not isinstance(y[0], list):\n",
    "            y = [[_y] for _y in y]\n",
    "        result = scrolls_metric.compute(predictions=p, references=y)\n",
    "        for metric_name in task_to_metric[args.task_name]:\n",
    "            metrics[metric_name] = result[metric_name]\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = list(range(150))\n",
    "# block_size = 10\n",
    "# history_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = [input_ids[max({0, start - block_size - history_size}): start] for start in range(block_size, len(input_ids), block_size)]\n",
    "# [len(s) for s in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'pid', 'input', 'output'],\n",
       "    num_rows: 2567\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'input_ids_generate', 'labels_mask', 'attention_mask', 'attention_mask_generate', 'id', 'target_text'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gen=iter(train_dataloader)\n",
    "train_batch = next(train_gen)\n",
    "train_batch = next(train_gen)\n",
    "train_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'pid', 'input', 'output'],\n",
       "    num_rows: 2567\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = list(map(tokenizer.encode, train_dataset['output'] + valid_dataset['output']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(e) for e in encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('wikitext', 'wikitext-2-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1147.4375"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "36718 / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = list(map(tokenizer.encode, ds['train']['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(t) for t in tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.4344e+04, 6.4920e+03, 3.9290e+03, 1.4210e+03, 4.0700e+02,\n",
       "        8.3000e+01, 3.0000e+01, 6.0000e+00, 2.0000e+00, 4.0000e+00]),\n",
       " array([  0. ,  81.5, 163. , 244.5, 326. , 407.5, 489. , 570.5, 652. ,\n",
       "        733.5, 815. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591.42578125"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lens) / 128 / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.9752709842584"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2061,   389,   262,  2482,    30,   198,   198, 21906,   198,    32,\n",
       "          4812,   425,  2995,   347,  9865, 31688,    15,   389,  2995,   326,\n",
       "          6032,  2689,   661,   287,  3967,   393,  4633,  2842,    13,  1114,\n",
       "          1672,    11,  1972,  1637,   290,  2712,  5701,   389,  3221,  3967,\n",
       "           284,   262,  3410,  7999,    26, 16508,  4692,   290,  6078,   530,\n",
       "           338, 13008,   389,  4633,    13, 28491,  2689,   425,  2995,   318,\n",
       "          1593,   284,  2972,  3288,  3303,  7587,   357,    45, 19930,     8,\n",
       "          5479,   884,   355, 10721,  3341,   347,  9865, 31688,    16,    11,\n",
       "          1808,    12,   504,    86,  1586,  3341,   347,  9865, 31688,    17,\n",
       "            11,   290, 14733,  9465,   347,  9865, 31688,    18,    13,   554,\n",
       "           428,  3348,    11,   356,   670,   319, 22650,   262,   755,  6806,\n",
       "           286,   281,  2689,   425,  1785,   326,   318,  7997,   416,   257,\n",
       "          4776, 12897,   422,   720,    12,    16,     3, 50257, 12814,   477,\n",
       "          1366,   284,  4512,    25,  8355,  1377,  8436, 10761,    52,  8793,\n",
       "           657,    13,    23,  3559,  9922,    11,  8355,  1377,   347, 17395,\n",
       "          8793,   657,    13,  4521,    18,  9922,    11,  8355,    10,  8141,\n",
       "            10,  8220,  1377,  8436, 10761,    52,  8793,   657,    13, 42240,\n",
       "          9922,    11,  8355,    10,  8141,    10,  8220,  1377,   347, 17395,\n",
       "          8793,   657,    13,    23,  2327,    11,  9922,    11,  7125,    47,\n",
       "          1377,  8436, 10761,    52,  8793,   657,    13,    24,  1129,  9922,\n",
       "            11,  7125,    47,  1377,   347, 17395,   257,   354,  1572,   657,\n",
       "            13,    24,  2091,    11,  9922,    11,  7125,    47,    10,  1847,\n",
       "            10,  8141,    10,  8220,  1377,  8436, 10761,    52,  8793,   657,\n",
       "            13,    24,  1558,  9922,    11,  7125,    47,    10,  1847,    10,\n",
       "          8141,    10,  8220,  1377,   347, 17395,  8793,   657,    13,    24,\n",
       "          1485,  9922,    13,   220,   198, 12814],\n",
       "        [ 2437,   389,  2316,   973,   284, 47933,   755,  6806,    30,   198,\n",
       "           198, 21906,   198,    32,  4812,   425,  2995,   347,  9865, 31688,\n",
       "            15,   389,  2995,   326,  6032,  2689,   661,   287,  3967,   393,\n",
       "          4633,  2842,    13,  1114,  1672,    11,  1972,  1637,   290,  2712,\n",
       "          5701,   389,  3221,  3967,   284,   262,  3410,  7999,    26, 16508,\n",
       "          4692,   290,  6078,   530,   338, 13008,   389,  4633,    13, 28491,\n",
       "          2689,   425,  2995,   318,  1593,   284,  2972,  3288,  3303,  7587,\n",
       "           357,    45, 19930,     8,  5479,   884,   355, 10721,  3341,   347,\n",
       "          9865, 31688,    16,    11,  1808,    12,   504,    86,  1586,  3341,\n",
       "           347,  9865, 31688,    17,    11,   290, 14733,  9465,   347,  9865,\n",
       "         31688,    18, 50257,  3106,   319,   262,  8695,  1022,  2995,    11,\n",
       "           262,  5220,   755,  6806,   286,   530,  1785,   460,  5004,   262,\n",
       "          1744,   755,  6806,   286,   262,   584,  1785,   220, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2061,   389,   262,  2482,    30,   198,   198, 21906,   198,    32,\n",
       "          4812,   425,  2995,   347,  9865, 31688,    15,   389,  2995,   326,\n",
       "          6032,  2689,   661,   287,  3967,   393,  4633,  2842,    13,  1114,\n",
       "          1672,    11,  1972,  1637,   290,  2712,  5701,   389,  3221,  3967,\n",
       "           284,   262,  3410,  7999,    26, 16508,  4692,   290,  6078,   530,\n",
       "           338, 13008,   389,  4633,    13, 28491,  2689,   425,  2995,   318,\n",
       "          1593,   284,  2972,  3288,  3303,  7587,   357,    45, 19930,     8,\n",
       "          5479,   884,   355, 10721,  3341,   347,  9865, 31688,    16,    11,\n",
       "          1808,    12,   504,    86,  1586,  3341,   347,  9865, 31688,    17,\n",
       "            11,   290, 14733,  9465,   347,  9865, 31688,    18,    13,   554,\n",
       "           428,  3348,    11,   356,   670,   319, 22650,   262,   755,  6806,\n",
       "           286,   281,  2689,   425,  1785,   326,   318,  7997,   416,   257,\n",
       "          4776, 12897,   422,   720,    12,    16,     3, 50257],\n",
       "        [ 2437,   389,  2316,   973,   284, 47933,   755,  6806,    30,   198,\n",
       "           198, 21906,   198,    32,  4812,   425,  2995,   347,  9865, 31688,\n",
       "            15,   389,  2995,   326,  6032,  2689,   661,   287,  3967,   393,\n",
       "          4633,  2842,    13,  1114,  1672,    11,  1972,  1637,   290,  2712,\n",
       "          5701,   389,  3221,  3967,   284,   262,  3410,  7999,    26, 16508,\n",
       "          4692,   290,  6078,   530,   338, 13008,   389,  4633,    13, 28491,\n",
       "          2689,   425,  2995,   318,  1593,   284,  2972,  3288,  3303,  7587,\n",
       "           357,    45, 19930,     8,  5479,   884,   355, 10721,  3341,   347,\n",
       "          9865, 31688,    16,    11,  1808,    12,   504,    86,  1586,  3341,\n",
       "           347,  9865, 31688,    17,    11,   290, 14733,  9465,   347,  9865,\n",
       "         31688,    18, 50257, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
       "         50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch['input_ids_generate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What are the results?\\n\\nIntroduction\\nAffective events BIBREF0 are events that typically affect people in positive or negative ways. For example, getting money and playing sports are usually positive to the experiencers; catching cold and losing one's wallet are negative. Understanding affective events is important to various natural language processing (NLP) applications such as dialogue systems BIBREF1, question-answering systems BIBREF2, and humor recognition BIBREF3. In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$[GEN]Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \\nUsing\",\n",
       " \"How are relations used to propagate polarity?\\n\\nIntroduction\\nAffective events BIBREF0 are events that typically affect people in positive or negative ways. For example, getting money and playing sports are usually positive to the experiencers; catching cold and losing one's wallet are negative. Understanding affective events is important to various natural language processing (NLP) applications such as dialogue systems BIBREF1, question-answering systems BIBREF2, and humor recognition BIBREF3[GEN]based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event [PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(train_batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch['labels_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(86.1150, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = train_batch.pop('id')\n",
    "target_text = train_batch.pop('target_text')\n",
    "input_ids_generate = train_batch.pop('input_ids_generate')\n",
    "attention_mask_generate = train_batch.pop('attention_mask_generate')\n",
    "\n",
    "out = model(**train_batch)\n",
    "out.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is the seed lexicon?\\n\\nIntroduction\\nAffective events BIBREF0 are events that typically affect people in positive or negative ways. For example, getting money and playing sports are usually positive to the experiencers; catching cold and losing one's wallet are negative. Understanding affective events is important to various natural language processing (NLP) applications such as dialogue systems BIBREF1, question-answering systems BIBREF2, and humor recognition BIBREF3. In this paper, we work on recognizing[GEN]a vocabulary of positive and negative predicates that helps determine the polarity score of an event\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_batch['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '753990d0b621d390ed58f20c4d9e4f065f0dc672',\n",
       " 'pid': '753990d0b621d390ed58f20c4d9e4f065f0dc672_0',\n",
       " 'input': 'What is the seed lexicon?\\n\\nIntroduction\\nAffective events BIBREF0 are events that typically affect people in positive or negative ways. For example, getting money and playing sports are usually positive to the experiencers; catching cold and losing one\\'s wallet are negative. Understanding affective events is important to various natural language processing (NLP) applications such as dialogue systems BIBREF1, question-answering systems BIBREF2, and humor recognition BIBREF3. In this paper, we work on recognizing the polarity of an affective event that is represented by a score ranging from $-1$ (negative) to 1 (positive).\\nLearning affective events is challenging because, as the examples above suggest, the polarity of an event is not necessarily predictable from its constituent words. Combined with the unbounded combinatorial nature of language, the non-compositionality of affective polarity entails the need for large amounts of world knowledge, which can hardly be learned from small annotated data.\\nIn this paper, we propose a simple and effective method for learning affective events that only requires a very small seed lexicon and a large raw corpus. As illustrated in Figure FIGREF1, our key idea is that we can exploit discourse relations BIBREF4 to efficiently propagate polarity from seed predicates that directly report one\\'s emotions (e.g., â€œto be gladâ€ is positive). Suppose that events $x_1$ are $x_2$ are in the discourse relation of Cause (i.e., $x_1$ causes $x_2$). If the seed lexicon suggests $x_2$ is positive, $x_1$ is also likely to be positive because it triggers the positive emotion. The fact that $x_2$ is known to be negative indicates the negative polarity of $x_1$. Similarly, if $x_1$ and $x_2$ are in the discourse relation of Concession (i.e., $x_2$ in spite of $x_1$), the reverse of $x_2$\\'s polarity can be propagated to $x_1$. Even if $x_2$\\'s polarity is not known in advance, we can exploit the tendency of $x_1$ and $x_2$ to be of the same polarity (for Cause) or of the reverse polarity (for Concession) although the heuristic is not exempt from counterexamples. We transform this idea into objective functions and train neural network models that predict the polarity of a given event.\\nWe trained the models using a Japanese web corpus. Given the minimum amount of supervision, they performed well. In addition, the combination of annotated and unannotated data yielded a gain over a purely supervised baseline when labeled data were small.\\nRelated Work\\nLearning affective events is closely related to sentiment analysis. Whereas sentiment analysis usually focuses on the polarity of what are described (e.g., movies), we work on how people are typically affected by events. In sentiment analysis, much attention has been paid to compositionality. Word-level polarity BIBREF5, BIBREF6, BIBREF7 and the roles of negation and intensification BIBREF8, BIBREF6, BIBREF9 are among the most important topics. In contrast, we are more interested in recognizing the sentiment polarity of an event that pertains to commonsense knowledge (e.g., getting money and catching cold).\\nLabel propagation from seed instances is a common approach to inducing sentiment polarities. While BIBREF5 and BIBREF10 worked on word- and phrase-level polarities, BIBREF0 dealt with event-level polarities. BIBREF5 and BIBREF10 linked instances using co-occurrence information and/or phrase-level coordinations (e.g., â€œ$A$ and $B$â€ and â€œ$A$ but $B$â€). We shift our scope to event pairs that are more complex than phrase pairs, and consequently exploit discourse connectives as event-level counterparts of phrase-level conjunctions.\\nBIBREF0 constructed a network of events using word embedding-derived similarities. Compared with this method, our discourse relation-based linking of events is much simpler and more intuitive.\\nSome previous studies made use of document structure to understand the sentiment. BIBREF11 proposed a sentiment-specific pre-training strategy using unlabeled dialog data (tweet-reply pairs). BIBREF12 proposed a method of building a polarity-tagged corpus (ACP Corpus). They automatically gathered sentences that had positive or negative opinions utilizing HTML layout structures in addition to linguistic patterns. Our method depends only on raw texts and thus has wider applicability.\\nProposed Method\\nProposed Method ::: Polarity Function\\nOur goal is to learn the polarity function $p(x)$, which predicts the sentiment polarity score of an event $x$. We approximate $p(x)$ by a neural network with the following form:\\n${\\\\rm Encoder}$ outputs a vector representation of the event $x$. ${\\\\rm Linear}$ is a fully-connected layer and transforms the representation into a scalar. ${\\\\rm tanh}$ is the hyperbolic tangent and transforms the scalar into a score ranging from $-1$ to 1. In Section SECREF21, we consider two specific implementations of ${\\\\rm Encoder}$.\\nProposed Method ::: Discourse Relation-Based Event Pairs\\nOur method requires a very small seed lexicon and a large raw corpus. We assume that we can automatically extract discourse-tagged event pairs, $(x_{i1}, x_{i2})$ ($i=1, \\\\cdots $) from the raw corpus. We refer to $x_{i1}$ and $x_{i2}$ as former and latter events, respectively. As shown in Figure FIGREF1, we limit our scope to two discourse relations: Cause and Concession.\\nThe seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. We expect the model to automatically learn complex phenomena through label propagation. Based on the availability of scores and the types of discourse relations, we classify the extracted event pairs into the following three types.\\nProposed Method ::: Discourse Relation-Based Event Pairs ::: AL (Automatically Labeled Pairs)\\nThe seed lexicon matches (1) the latter event but (2) not the former event, and (3) their discourse relation type is Cause or Concession. If the discourse relation type is Cause, the former event is given the same score as the latter. Likewise, if the discourse relation type is Concession, the former event is given the opposite of the latter\\'s score. They are used as reference scores during training.\\nProposed Method ::: Discourse Relation-Based Event Pairs ::: CA (Cause Pairs)\\nThe seed lexicon matches neither the former nor the latter event, and their discourse relation type is Cause. We assume the two events have the same polarities.\\nProposed Method ::: Discourse Relation-Based Event Pairs ::: CO (Concession Pairs)\\nThe seed lexicon matches neither the former nor the latter event, and their discourse relation type is Concession. We assume the two events have the reversed polarities.\\nProposed Method ::: Loss Functions\\nUsing AL, CA, and CO data, we optimize the parameters of the polarity function $p(x)$. We define a loss function for each of the three types of event pairs and sum up the multiple loss functions.\\nWe use mean squared error to construct loss functions. For the AL data, the loss function is defined as:\\nwhere $x_{i1}$ and $x_{i2}$ are the $i$-th pair of the AL data. $r_{i1}$ and $r_{i2}$ are the automatically-assigned scores of $x_{i1}$ and $x_{i2}$, respectively. $N_{\\\\rm AL}$ is the total number of AL pairs, and $\\\\lambda _{\\\\rm AL}$ is a hyperparameter.\\nFor the CA data, the loss function is defined as:\\n$y_{i1}$ and $y_{i2}$ are the $i$-th pair of the CA pairs. $N_{\\\\rm CA}$ is the total number of CA pairs. $\\\\lambda _{\\\\rm CA}$ and $\\\\mu $ are hyperparameters. The first term makes the scores of the two events closer while the second term prevents the scores from shrinking to zero.\\nThe loss function for the CO data is defined analogously:\\nThe difference is that the first term makes the scores of the two events distant from each other.\\nExperiments\\nExperiments ::: Dataset\\nExperiments ::: Dataset ::: AL, CA, and CO\\nAs a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. To extract event pairs tagged with discourse relations, we used the Japanese dependency parser KNP and in-house postprocessing scripts BIBREF14. KNP used hand-written rules to segment each sentence into what we conventionally called clauses (mostly consecutive text chunks), each of which contained one main predicate. KNP also identified the discourse relations of event pairs if explicit discourse connectives BIBREF4 such as â€œã®ã§â€ (because) and â€œã®ã«â€ (in spite of) were present. We treated Cause/Reason (åŽŸå› ãƒ»ç†ç”±) and Condition (æ¡ä»¶) in the original tagset BIBREF15 as Cause and Concession (é€†æŽ¥) as Concession, respectively. Here is an example of event pair extraction.\\n. é‡å¤§ãªå¤±æ•—ã‚’çŠ¯ã—ãŸã®ã§ã€ä»•äº‹ã‚’ã‚¯ãƒ“ã«ãªã£ãŸã€‚\\nBecause [I] made a serious mistake, [I] got fired.\\nFrom this sentence, we extracted the event pair of â€œé‡å¤§ãªå¤±æ•—ã‚’çŠ¯ã™â€ ([I] make a serious mistake) and â€œä»•äº‹ã‚’ã‚¯ãƒ“ã«ãªã‚‹â€ ([I] get fired), and tagged it with Cause.\\nWe constructed our seed lexicon consisting of 15 positive words and 15 negative words, as shown in Section SECREF27. From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO. We randomly selected subsets of AL event pairs such that positive and negative latter events were equal in size. We also sampled event pairs for each of CA and CO such that it was five times larger than AL. The results are shown in Table TABREF16.\\nExperiments ::: Dataset ::: ACP (ACP Corpus)\\nWe used the latest version of the ACP Corpus BIBREF12 for evaluation. It was used for (semi-)supervised training as well. Extracted from Japanese websites using HTML layouts and linguistic patterns, the dataset covered various genres. For example, the following two sentences were labeled positive and negative, respectively:\\n. ä½œæ¥­ãŒæ¥½ã ã€‚\\nThe work is easy.\\n. é§è»Šå ´ãŒãªã„ã€‚\\nThere is no parking lot.\\nAlthough the ACP corpus was originally constructed in the context of sentiment analysis, we found that it could roughly be regarded as a collection of affective events. We parsed each sentence and extracted the last clause in it. The train/dev/test split of the data is shown in Table TABREF19.\\nThe objective function for supervised training is:\\nwhere $v_i$ is the $i$-th event, $R_i$ is the reference score of $v_i$, and $N_{\\\\rm ACP}$ is the number of the events of the ACP Corpus.\\nTo optimize the hyperparameters, we used the dev set of the ACP Corpus. For the evaluation, we used the test set of the ACP Corpus. The model output was classified as positive if $p(x) > 0$ and negative if $p(x) \\\\le 0$.\\nExperiments ::: Model Configurations\\nAs for ${\\\\rm Encoder}$, we compared two types of neural networks: BiGRU and BERT. GRU BIBREF16 is a recurrent neural network sequence encoder. BiGRU reads an input sequence forward and backward and the output is the concatenation of the final forward and backward hidden states.\\nBERT BIBREF17 is a pre-trained multi-layer bidirectional Transformer BIBREF18 encoder. Its output is the final hidden state corresponding to the special classification tag ([CLS]). For the details of ${\\\\rm Encoder}$, see Sections SECREF30.\\nWe trained the model with the following four combinations of the datasets: AL, AL+CA+CO (two proposed models), ACP (supervised), and ACP+AL+CA+CO (semi-supervised). The corresponding objective functions were: $\\\\mathcal {L}_{\\\\rm AL}$, $\\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$, $\\\\mathcal {L}_{\\\\rm ACP}$, and $\\\\mathcal {L}_{\\\\rm ACP} + \\\\mathcal {L}_{\\\\rm AL} + \\\\mathcal {L}_{\\\\rm CA} + \\\\mathcal {L}_{\\\\rm CO}$.\\nExperiments ::: Results and Discussion\\nTable TABREF23 shows accuracy. As the Random baseline suggests, positive and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon and output the corresponding label (or the reverse of it for negation) if the event\\'s predicate is in the seed lexicon. We can see that the seed lexicon itself had practically no impact on prediction.\\nThe models in the top block performed considerably better than the random baselines. The performance gaps with their (semi-)supervised counterparts, shown in the middle block, were less than 7%. This demonstrates the effectiveness of discourse relation-based label propagation.\\nComparing the model variants, we obtained the highest score with the BiGRU encoder trained with the AL+CA+CO dataset. BERT was competitive but its performance went down if CA and CO were used in addition to AL. We conjecture that BERT was more sensitive to noises found more frequently in CA and CO.\\nContrary to our expectations, supervised models (ACP) outperformed semi-supervised models (ACP+AL+CA+CO). This suggests that the training set of 0.6 million events is sufficiently large for training the models. For comparison, we trained the models with a subset (6,000 events) of the ACP dataset. As the results shown in Table TABREF24 demonstrate, our method is effective when labeled data are small.\\nThe result of hyperparameter optimization for the BiGRU encoder was as follows:\\nAs the CA and CO pairs were equal in size (Table TABREF16), $\\\\lambda _{\\\\rm CA}$ and $\\\\lambda _{\\\\rm CO}$ were comparable values. $\\\\lambda _{\\\\rm CA}$ was about one-third of $\\\\lambda _{\\\\rm CO}$, and this indicated that the CA pairs were noisier than the CO pairs. A major type of CA pairs that violates our assumption was in the form of â€œ$\\\\textit {problem}_{\\\\text{negative}}$ causes $\\\\textit {solution}_{\\\\text{positive}}$â€:\\n. (æ‚ªã„ã¨ã“ã‚ãŒã‚ã‚‹, ã‚ˆããªã‚‹ã‚ˆã†ã«åŠªåŠ›ã™ã‚‹)\\n(there is a bad point, [I] try to improve [it])\\nThe polarities of the two events were reversed in spite of the Cause relation, and this lowered the value of $\\\\lambda _{\\\\rm CA}$.\\nSome examples of model outputs are shown in Table TABREF26. The first two examples suggest that our model successfully learned negation without explicit supervision. Similarly, the next two examples differ only in voice but the model correctly recognized that they had opposite polarities. The last two examples share the predicate â€œè½ã¨ã™\" (drop) and only the objects are different. The second event â€œè‚©ã‚’è½ã¨ã™\" (lit. drop one\\'s shoulders) is an idiom that expresses a disappointed feeling. The examples demonstrate that our model correctly learned non-compositional expressions.\\nConclusion\\nIn this paper, we proposed to use discourse relations to effectively propagate polarities of affective events from seeds. Experiments show that, even with a minimal amount of supervision, the proposed method performed well.\\nAlthough event pairs linked by discourse analysis are shown to be useful, they nevertheless contain noises. Adding linguistically-motivated filtering rules would help improve the performance.\\nAcknowledgments\\nWe thank Nobuhiro Kaji for providing the ACP Corpus and Hirokazu Kiyomaru and Yudai Kishimoto for their help in extracting event pairs. This work was partially supported by Yahoo! Japan Corporation.\\nAppendices ::: Seed Lexicon ::: Positive Words\\nå–œã¶ (rejoice), å¬‰ã—ã„ (be glad), æ¥½ã—ã„ (be pleasant), å¹¸ã› (be happy), æ„Ÿå‹• (be impressed), èˆˆå¥® (be excited), æ‡ã‹ã—ã„ (feel nostalgic), å¥½ã (like), å°Šæ•¬ (respect), å®‰å¿ƒ (be relieved), æ„Ÿå¿ƒ (admire), è½ã¡ç€ã (be calm), æº€è¶³ (be satisfied), ç™’ã•ã‚Œã‚‹ (be healed), and ã‚¹ãƒƒã‚­ãƒª (be refreshed).\\nAppendices ::: Seed Lexicon ::: Negative Words\\næ€’ã‚‹ (get angry), æ‚²ã—ã„ (be sad), å¯‚ã—ã„ (be lonely), æ€–ã„ (be scared), ä¸å®‰ (feel anxious), æ¥ãšã‹ã—ã„ (be embarrassed), å«Œ (hate), è½ã¡è¾¼ã‚€ (feel down), é€€å±ˆ (be bored), çµ¶æœ› (feel hopeless), è¾›ã„ (have a hard time), å›°ã‚‹ (have trouble), æ†‚é¬± (be depressed), å¿ƒé… (be worried), and æƒ…ã‘ãªã„ (be sorry).\\nAppendices ::: Settings of Encoder ::: BiGRU\\nThe dimension of the embedding layer was 256. The embedding layer was initialized with the word embeddings pretrained using the Web corpus. The input sentences were segmented into words by the morphological analyzer Juman++. The vocabulary size was 100,000. The number of hidden layers was 2. The dimension of hidden units was 256. The optimizer was Momentum SGD BIBREF21. The mini-batch size was 1024. We ran 100 epochs and selected the snapshot that achieved the highest score for the dev set.\\nAppendices ::: Settings of Encoder ::: BERT\\nWe used a Japanese BERT model pretrained with Japanese Wikipedia. The input sentences were segmented into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary size was 32,000. The maximum length of an input sequence was 128. The number of hidden layers was 12. The dimension of hidden units was 768. The number of self-attention heads was 12. The optimizer was Adam BIBREF19. The mini-batch size was 32. We ran 1 epoch.',\n",
       " 'output': 'a vocabulary of positive and negative predicates that helps determine the polarity score of an event'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_for_metrics_fn(train_batch, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'input_ids_generate', 'labels_mask', 'attention_mask', 'id', 'target_text'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_gen=iter(train_dataloader)\n",
    "valid_batch = next(valid_gen)\n",
    "valid_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = valid_batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/jovyan/envs/accel_rmt/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gen_out = model.generate(input_ids, attention_mask=valid_batch['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>[GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN]',\n",
       " '<|endoftext|>[GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN][GEN]']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text = tokenizer.batch_decode(gen_out)\n",
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is the seed lexicon?\\n\\nIntroduction\\nAffective events BIBREF0 are events that typically affect people in positive or negative ways. For example, getting money and playing sports are usually positive to the experiencers; catching cold and losing one's wallet are negative. Understanding affective events is important to various natural language processing (NLP) applications such as dialogue systems BIBREF1, question-answering systems BIBREF2, and humor recognition BIBREF3. In this paper, we work on recognizing[GEN]a vocabulary of positive and negative predicates that helps determine the polarity score of an event\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_batch['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(**valid_batch)\n",
    "data['generation_outputs'] = gen_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.0, 'f1': 0.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_fn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensor.__contains__ only supports Tensor or scalar, but you passed in a <class 'str'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/rmt/wip/notebooks/debug_rmt_scrolls_run.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Brmt-8a100-3/home/jovyan/rmt/wip/notebooks/debug_rmt_scrolls_run.ipynb#Y404sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m keep_for_metrics_fn(valid_batch, gen_out)\n",
      "\u001b[1;32m/home/jovyan/rmt/wip/notebooks/debug_rmt_scrolls_run.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brmt-8a100-3/home/jovyan/rmt/wip/notebooks/debug_rmt_scrolls_run.ipynb#Y404sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mkeep_for_metrics_fn\u001b[39m(batch, output):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brmt-8a100-3/home/jovyan/rmt/wip/notebooks/debug_rmt_scrolls_run.ipynb#Y404sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# select data from batch and model output that would be used to compute metrics\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brmt-8a100-3/home/jovyan/rmt/wip/notebooks/debug_rmt_scrolls_run.ipynb#Y404sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     data \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Brmt-8a100-3/home/jovyan/rmt/wip/notebooks/debug_rmt_scrolls_run.ipynb#Y404sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39;49m\u001b[39mgeneration_outputs\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39min\u001b[39;49;00m output:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brmt-8a100-3/home/jovyan/rmt/wip/notebooks/debug_rmt_scrolls_run.ipynb#Y404sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m         data[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mtarget_text\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brmt-8a100-3/home/jovyan/rmt/wip/notebooks/debug_rmt_scrolls_run.ipynb#Y404sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m         data[\u001b[39m'\u001b[39m\u001b[39mgeneration_outputs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39m\u001b[39mgeneration_outputs\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/envs/accel_rmt/lib/python3.9/site-packages/torch/_tensor.py:999\u001b[0m, in \u001b[0;36mTensor.__contains__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(element, (torch\u001b[39m.\u001b[39mTensor, Number)):\n\u001b[1;32m    996\u001b[0m     \u001b[39m# type hint doesn't understand the __contains__ result array\u001b[39;00m\n\u001b[1;32m    997\u001b[0m     \u001b[39mreturn\u001b[39;00m (element \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39many()\u001b[39m.\u001b[39mitem()  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 999\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1000\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTensor.__contains__ only supports Tensor or scalar, but you passed in a \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1001\u001b[0m     \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(element)\n\u001b[1;32m   1002\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor.__contains__ only supports Tensor or scalar, but you passed in a <class 'str'>."
     ]
    }
   ],
   "source": [
    "keep_for_metrics_fn(valid_batch, gen_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
